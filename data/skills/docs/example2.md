# AI Research Skills í™•ì¥ ì˜ˆì‹œ ë¶ 2 (Creative Cookbook)

> ì°½ì˜ì ì´ê³  ê³ ë„í™”ëœ AI ì‘ìš© ì‹œë‚˜ë¦¬ì˜¤ ë° êµ¬í˜„ ê°€ì´ë“œ

---

## ğŸ“š ëª©ì°¨

### Part F: ì°½ì˜ì  ë©€í‹°ëª¨ë‹¬ ì‘ìš©
- [F1. AI ë”¥ë‹¤ì´ë¸Œ íŒŸìºìŠ¤íŠ¸ ìƒì„±ê¸°](#f1-ai-ë”¥ë‹¤ì´ë¸Œ-íŒŸìºìŠ¤íŠ¸-ìƒì„±ê¸°)
- [F2. UX/UI ìë™ ì§„ë‹¨ ë° ê°œì„  ì—ì´ì „íŠ¸](#f2-uxui-ìë™-ì§„ë‹¨-ë°-ê°œì„ -ì—ì´ì „íŠ¸)

### Part G: í•˜ì´í¼ ì˜¤í† ë©”ì´ì…˜ (Hyper-Automation)
- [G1. "The Midnight Coder" ììœ¨ ë¦¬íŒ©í† ë§](#g1-the-midnight-coder-ììœ¨-ë¦¬íŒ©í† ë§)
- [G2. ì‹¤ì‹œê°„ ìœ„ê¸° ëŒ€ì‘ ìƒí™©ì‹¤](#g2-ì‹¤ì‹œê°„-ìœ„ê¸°-ëŒ€ì‘-ìƒí™©ì‹¤)

### Part H: ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ & ë¶„ì„ ì‹¬í™”
- [H1. ì¸ê³¼ ì¶”ë¡  (Causal Inference) ë§ˆì¼€íŒ… ë¶„ì„ê¸°](#h1-ì¸ê³¼-ì¶”ë¡ -ë§ˆì¼€íŒ…-ë¶„ì„ê¸°)
- [H2. "Auto-Kaggle" ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸](#h2-auto-kaggle-ëª¨ë¸ë§-íŒŒì´í”„ë¼ì¸)

### Part I: í‹ˆìƒˆ ë„ë©”ì¸ íŠ¹í™”
- [I1. íŠ¹í—ˆ ì¹¨í•´ ê°€ëŠ¥ì„± ë¶„ì„ê¸°](#i1-íŠ¹í—ˆ-ì¹¨í•´-ê°€ëŠ¥ì„±-ë¶„ì„ê¸°)
- [I2. ê°œì¸í™”ëœ "Second Brain" ì§€ì‹ ê·¸ë˜í”„](#i2-ê°œì¸í™”ëœ-second-brain-ì§€ì‹-ê·¸ë˜í”„)

---

# Part F: ì°½ì˜ì  ë©€í‹°ëª¨ë‹¬ ì‘ìš©

## F1. AI ë”¥ë‹¤ì´ë¸Œ íŒŸìºìŠ¤íŠ¸ ìƒì„±ê¸°

### ì‹œë‚˜ë¦¬ì˜¤: "ë…¼ë¬¸ ì½ì–´ì£¼ëŠ” ë‘ ì¹œêµ¬"

**ëª©í‘œ**: ê¸°ìˆ  ë…¼ë¬¸(PDF)ì´ë‚˜ ë³µì¡í•œ ë¬¸ì„œë¥¼ ì…ë ¥í•˜ë©´, ë‘ ëª…ì˜ AI í˜¸ìŠ¤íŠ¸(ì§„í–‰ì & ì „ë¬¸ê°€)ê°€ ì‰½ê³  ì¬ë¯¸ìˆê²Œ ëŒ€í™”í•˜ëŠ” 10ë¶„ ë¶„ëŸ‰ì˜ ì˜¤ë””ì˜¤ ì½˜í…ì¸  ìƒì„±. (Google NotebookLM ìŠ¤íƒ€ì¼)

**ì‚¬ìš© ìŠ¤í‚¬**: `18-multimodal`, `16-prompt-engineering`, `14-agents`

#### êµ¬í˜„ íŒŒì´í”„ë¼ì¸

1.  **PDF íŒŒì‹±**: ë…¼ë¬¸ í…ìŠ¤íŠ¸ ë° êµ¬ì¡° ì¶”ì¶œ
2.  **ëŒ€ë³¸ ì‘ì„± (Script Writer)**: ë‘ í˜ë¥´ì†Œë‚˜ì˜ ëŒ€í™” ìƒì„± (ìœ ë¨¸, ë¹„ìœ  í¬í•¨)
3.  **ìŒì„± í•©ì„± (TTS)**: ê° í™”ìë³„ ë‹¤ë¥¸ ëª©ì†Œë¦¬ë¡œ ì˜¤ë””ì˜¤ ìƒì„±
4.  **ì˜¤ë””ì˜¤ ë¯¹ì‹±**: ë°°ê²½ìŒì•… ë° íš¨ê³¼ìŒ ì‚½ì…

```python
import asyncio
from langchain_openai import ChatOpenAI
from langchain_community.document_loaders import PyPDFLoader
# ê°€ìƒ TTS ë¼ì´ë¸ŒëŸ¬ë¦¬ (OpenAI TTS or ElevenLabs)
from tts_provider import generate_speech, mix_audio

class PodcastProducer:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.7)
        
    async def produce_episode(self, pdf_path: str, output_file: str):
        # 1. ë¬¸ì„œ ë¶„ì„
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        content = "\n".join([d.page_content for d in docs[:5]]) # ì•ë¶€ë¶„ë§Œ ì˜ˆì‹œ
        
        # 2. ëŒ€ë³¸ ìƒì„± (í˜ë¥´ì†Œë‚˜ ì •ì˜ ì¤‘ìš”)
        script = await self.generate_script(content)
        
        # 3. ìŒì„± í•©ì„± (ë³‘ë ¬ ì²˜ë¦¬)
        audio_segments = []
        for line in script:
            voice_id = "alloy" if line["speaker"] == "Host" else "onyx"
            audio = await generate_speech(
                text=line["text"], 
                voice=voice_id,
                emotion=line.get("emotion", "neutral")
            )
            audio_segments.append(audio)
            
        # 4. ë¯¹ì‹± (Intro/Outro BGM ì¶”ê°€)
        final_audio = mix_audio(
            segments=audio_segments,
            bgm="lofi_beat.mp3",
            bgm_volume=0.1
        )
        final_audio.export(output_file, format="mp3")
        
    async def generate_script(self, content: str):
        """DSPy ìŠ¤íƒ€ì¼ í”„ë¡¬í”„íŒ…ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™” ìƒì„±"""
        prompt = f"""
        ë‹¹ì‹ ì€ ì¸ê¸° í…Œí¬ íŒŸìºìŠ¤íŠ¸ì˜ PDì…ë‹ˆë‹¤. 
        ì£¼ì–´ì§„ ê¸°ìˆ  ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‘ ì§„í–‰ì(Alex, Jamie)ì˜ ëŒ€ë³¸ì„ ì‘ì„±í•˜ì„¸ìš”.
        
        [í˜ë¥´ì†Œë‚˜]
        - **Alex (ì§„í–‰ì)**: í˜¸ê¸°ì‹¬ ë§ê³  ì—ë„ˆì§€ê°€ ë„˜ì¹¨. ì§ˆë¬¸ì„ ë˜ì§€ê³  ì²­ì·¨ìì˜ ëˆˆë†’ì´ì—ì„œ ë¹„ìœ ë¥¼ ì‚¬ìš©í•¨.
        - **Jamie (ì „ë¬¸ê°€)**: ì°¨ë¶„í•˜ê³  ì§€ì ì„. í•µì‹¬ ì›ë¦¬ë¥¼ ëª…ì¾Œí•˜ê²Œ ì„¤ëª…í•˜ê³  ê¹Šì´ ìˆëŠ” í†µì°°ì„ ì œê³µí•¨.
        
        [ê·œì¹™]
        1. "ì•ˆë…•í•˜ì„¸ìš”" ê°™ì€ ë»”í•œ ì¸ì‚¬ëŠ” ìƒëµí•˜ê³  ë°”ë¡œ ë³¸ë¡ ì˜ í¥ë¯¸ë¡œìš´ ì ìœ¼ë¡œ ì‹œì‘í•  ê²ƒ.
        2. ì¤‘ê°„ì— ê°€ë²¼ìš´ ë†ë‹´ì´ë‚˜ ê°íƒ„ì‚¬("ì™€, ì§„ì§œìš”?", "ì ê¹ë§Œìš”!")ë¥¼ ë„£ì–´ ìì—°ìŠ¤ëŸ½ê²Œ ë§Œë“¤ ê²ƒ.
        3. ë¬¸ì–´ì²´ê°€ ì•„ë‹Œ êµ¬ì–´ì²´ë¥¼ ì‚¬ìš©í•  ê²ƒ.
        4. JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•  ê²ƒ: {{"speaker": "Alex", "text": "...", "emotion": "excited"}}
        
        ë¬¸ì„œ ë‚´ìš©: {content[:3000]}...
        """
        # (êµ¬í˜„ ìƒëµ: LLM í˜¸ì¶œ ë° JSON íŒŒì‹±)
        return parsed_script_json
```

---

## F2. UX/UI ìë™ ì§„ë‹¨ ë° ê°œì„  ì—ì´ì „íŠ¸

### ì‹œë‚˜ë¦¬ì˜¤: "ë””ìì¸ ë‹¥í„°"

**ëª©í‘œ**: ì›¹ì‚¬ì´íŠ¸ URLì´ë‚˜ ìŠ¤í¬ë¦°ìƒ·ì„ ì…ë ¥í•˜ë©´, ì‚¬ìš©ì„± ë¬¸ì œ(Usability)ì™€ ì‹œê°ì  ê²°í•¨(Visual Glitch)ì„ ì§„ë‹¨í•˜ê³ , ê°œì„ ëœ CSS/React ì½”ë“œë¥¼ ì œì•ˆ.

**ì‚¬ìš© ìŠ¤í‚¬**: `18-multimodal` (Vision), `23-frontend-design-architect`

#### êµ¬í˜„

```python
from langchain_core.messages import HumanMessage
import base64

class DesignDoctor:
    def __init__(self):
        self.vision_model = ChatOpenAI(model="gpt-4o", max_tokens=2048)
        
    async def diagnose(self, image_path: str):
        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        with open(image_path, "rb") as f:
            img_base64 = base64.b64encode(f.read()).decode("utf-8")
            
        # ë¹„ì „ ëª¨ë¸ ë¶„ì„
        msg = HumanMessage(content=[
            {"type": "text", "text": """
            ë‹¹ì‹ ì€ ì‹œë‹ˆì–´ UX/UI ë””ìì´ë„ˆì…ë‹ˆë‹¤. ì´ ì›¹ì‚¬ì´íŠ¸ ìŠ¤í¬ë¦°ìƒ·ì„ ë¶„ì„í•˜ê³  ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:
            
            1. **íœ´ë¦¬ìŠ¤í‹± í‰ê°€**: ë‹ìŠ¨ì˜ 10ê°€ì§€ íœ´ë¦¬ìŠ¤í‹± ì›ì¹™ì— ìœ„ë°°ë˜ëŠ” ì  ì°¾ê¸° (ì˜ˆ: ê°€ì‹œì„± ë¶€ì¡±, ì¼ê´€ì„± ê²°ì—¬).
            2. **ì‹œê°ì  ê³„ì¸µ êµ¬ì¡°**: íƒ€ì´í¬ê·¸ë˜í”¼, ëŒ€ë¹„, ì—¬ë°±ì´ ì •ë³´ ì „ë‹¬ì— íš¨ê³¼ì ì¸ì§€ ë¶„ì„.
            3. **ì ‘ê·¼ì„±(a11y)**: ìƒ‰ìƒ ëŒ€ë¹„ê°€ ì¶©ë¶„í•œì§€, í„°ì¹˜ íƒ€ê²Ÿì´ ì ì ˆí•œì§€ ì¶”ì •.
            4. **ê°œì„  ì œì•ˆ**: Tailwind CSSë¥¼ ì‚¬ìš©í•œ êµ¬ì²´ì ì¸ ê°œì„  ì½”ë“œ ì œì•ˆ.
            """},
            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}}
        ])
        
        response = await self.vision_model.ainvoke([msg])
        return response.content

    def generate_report(self, diagnosis: str):
        # ë¶„ì„ ê²°ê³¼ë¥¼ ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ë¡œ ë³€í™˜
        # ê°œì„  ì „/í›„ ë¹„êµ ì½”ë“œ ë¸”ë¡ ê°•ì¡°
        pass
```

---

# Part G: í•˜ì´í¼ ì˜¤í† ë©”ì´ì…˜ (Hyper-Automation)

## G1. "The Midnight Coder" ììœ¨ ë¦¬íŒ©í† ë§

### ì‹œë‚˜ë¦¬ì˜¤: ë°¤ìƒ˜ ììœ¨ ë¦¬íŒ©í† ë§

**ëª©í‘œ**: ê°œë°œìê°€ í‡´ê·¼í•œ í›„(Midnight), CI íŒŒì´í”„ë¼ì¸ì´ ì‹¤í–‰ë˜ì–´ ë ˆê±°ì‹œ ì½”ë“œ(Python 2.7 ìŠ¤íƒ€ì¼, ë¹„íš¨ìœ¨ì  Pandas ë“±)ë¥¼ íƒì§€í•˜ê³ , ëª¨ë˜ Python(Type Hinting, Pydantic)ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ PRì„ ì˜¬ë¦¬ëŠ” ì‹œìŠ¤í…œ.

**ì‚¬ìš© ìŠ¤í‚¬**: `14-agents`, `07-safety-alignment`, `Github API`

#### ì•„í‚¤í…ì²˜

```
[Cron Job: 02:00 AM]
      â†“
[Code Scanner] â†’ (Static Analysis: SonarQube/Ruff)
      â†“
[Refactoring Agent] â†â†’ [Unit Test Runner]
      â†“                 (ìˆ˜ì • í›„ í…ŒìŠ¤íŠ¸ í†µê³¼ í•„ìˆ˜)
[PR Creator] â†’ "refactor/auto-fix-metadata" ë¸Œëœì¹˜ ìƒì„±
```

#### ì—ì´ì „íŠ¸ ë£¨í”„ êµ¬í˜„

```python
class RefactoringAgent:
    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4o")
        
    async def refactor_file(self, file_path: str, issues: list):
        original_code = read_file(file_path)
        
        # 1. ë¦¬íŒ©í† ë§ ì œì•ˆ
        prompt = f"""
        ë‹¤ìŒ Python ì½”ë“œë¥¼ ìµœì‹  í‘œì¤€ìœ¼ë¡œ ë¦¬íŒ©í† ë§í•˜ì„¸ìš”.
        ì´ìŠˆ: {issues}
        
        [ìš”êµ¬ì‚¬í•­]
        - Type Hinting ì¶”ê°€
        - Docstring (Google Style) ì¶”ê°€
        - Pydantic BaseModel ì‚¬ìš©í•˜ì—¬ ë°ì´í„° êµ¬ì¡°í™”
        - ë¹„íš¨ìœ¨ì ì¸ ë£¨í”„ë¥¼ List Comprehension ë˜ëŠ” Vectorizationìœ¼ë¡œ ë³€ê²½
        """
        
        new_code = await self.llm.invoke(prompt)
        
        # 2. ê²€ì¦ (Self-Correction Loop)
        for attempt in range(3):
            write_file(file_path, new_code)
            
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            test_result = run_pytest(file_path)
            
            if test_result.passed:
                return new_code
            
            # ì‹¤íŒ¨ ì‹œ ìˆ˜ì •
            new_code = await self.fix_code(
                original_code, new_code, test_result.error_log
            )
            
        # 3íšŒ ì‹¤íŒ¨ ì‹œ ë¡¤ë°±
        write_file(file_path, original_code)
        return None
```

## G2. ì‹¤ì‹œê°„ ìœ„ê¸° ëŒ€ì‘ ìƒí™©ì‹¤

### ì‹œë‚˜ë¦¬ì˜¤: ë¸Œëœë“œ í‰íŒ ë°©ì–´ ì‹œìŠ¤í…œ

**ëª©í‘œ**: íŠ¸ìœ„í„°, ì»¤ë®¤ë‹ˆí‹°, ë‰´ìŠ¤ë¥¼ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§í•˜ë‹¤ê°€ ë¸Œëœë“œì— ëŒ€í•œ ë¶€ì •ì  ì—¬ë¡ ì´ ê¸‰ì¦í•˜ë©´(Viral Spike), ì›ì¸ì„ ë¶„ì„í•˜ê³  ê³µì‹ ì…ì¥ë¬¸ ì´ˆì•ˆê³¼ ëŒ€ì‘ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ê²½ì˜ì§„ì—ê²Œ Push ì•Œë¦¼ìœ¼ë¡œ ì „ì†¡.

**ì‚¬ìš© ìŠ¤í‚¬**: `20-trading` (ì‹œê³„ì—´ ì´ìƒíƒì§€ ì‘ìš©), `21-multiagent`

#### ì›Œí¬í”Œë¡œìš°

1.  **Monitor Agent**: í‚¤ì›Œë“œ ì–¸ê¸‰ëŸ‰ ë° ê°ì„±ì§€ìˆ˜ ì‹¤ì‹œê°„ ì¶”ì  (Tradingì˜ RSI/ë³¼ë¦°ì € ë°´ë“œ ì§€í‘œ ì‘ìš©).
2.  **Alert Trigger**: ê°ì„±ì§€ìˆ˜ê°€ -2.0 SD(í‘œì¤€í¸ì°¨) ê¸‰ë½ ì‹œ "ìœ„ê¸°" ê²½ë³´ ë°œë ¹.
3.  **Analyst Agent**: ê¸‰ë½ ì›ì¸ì´ ëœ ìƒìœ„ ê²Œì‹œê¸€ í¬ë¡¤ë§ ë° íŒ©íŠ¸ ì²´í¬.
4.  **PR Agent**: ìœ„ê¸° ìœ í˜•(ì œí’ˆ ê²°í•¨, ì„ì› ë¦¬ìŠ¤í¬, ì˜¤ë³´ ë“±)ì— ë”°ë¥¸ ì‚¬ê³¼ë¬¸/í•´ëª…ë¬¸ ì´ˆì•ˆ ì‘ì„±.
5.  **Notification**: Slackìœ¼ë¡œ "ğŸš¨ ìœ„ê¸° ê°ì§€ ë¦¬í¬íŠ¸" ë°œì†¡.

```python
class CrisisWarRoom:
    async def monitor_stream(self):
        # ì‹¤ì‹œê°„ ë°ì´í„° ìŠ¤íŠ¸ë¦¼ (Kafka ë“±)
        async for data in self.social_stream:
            metrics = self.calculate_sentiment_metrics(data)
            
            # ì´ìƒ íƒì§€ (Z-score)
            if metrics['z_score'] < -3.0:
                await self.activate_protocol(data, metrics)
    
    async def activate_protocol(self, trigger_data, metrics):
        # ì›ì¸ ë¶„ì„
        top_posts = await self.analyst.find_viral_posts(hours=1)
        root_cause = await self.analyst.summarize_issue(top_posts)
        
        # ëŒ€ì‘ ì „ëµ ìˆ˜ë¦½
        strategy = await self.pr_agent.draft_strategy(
            issue=root_cause,
            severity="CRITICAL"
        )
        
        # ìŠ¬ë™ ì•Œë¦¼ ì „ì†¡
        await self.notifier.send_alert(
            title="ğŸ”´ ë¸Œëœë“œ ìœ„ê¸° ê²½ë³´ ë°œë ¹",
            fields=[
                {"title": "ê¸‰ë½ ì§€ìˆ˜", "value": f"{metrics['z_score']:.2f} sigma"},
                {"title": "í•µì‹¬ ì´ìŠˆ", "value": root_cause},
                {"title": "ì œì•ˆ ëŒ€ì‘", "value": strategy['action_plan']},
                {"title": "ì…ì¥ë¬¸ ì´ˆì•ˆ", "value": strategy['draft_text']}
            ]
        )
```

---

# Part H: ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ & ë¶„ì„ ì‹¬í™”

## H1. ì¸ê³¼ ì¶”ë¡  ë§ˆì¼€íŒ… ë¶„ì„ê¸°

### ì‹œë‚˜ë¦¬ì˜¤: "ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹ˆë‹¤"

**ëª©í‘œ**: ë‹¨ìˆœíˆ "ê´‘ê³ ë¹„ê°€ ëŠ˜ì–´ì„œ ë§¤ì¶œì´ ì˜¬ëë‹¤"ê°€ ì•„ë‹ˆë¼, "ê´‘ê³ ë¹„ë¥¼ 100ë§Œì› ëŠ˜ë ¸ì„ ë•Œ, ì™¸ë¶€ ìš”ì¸(ê³„ì ˆì„±, ê²½ìŸì‚¬ ê°€ê²©)ì„ í†µì œí•˜ê³  ìˆœìˆ˜í•˜ê²Œ ë§¤ì¶œì´ ì–¼ë§ˆë‚˜ ì˜¤ë¥´ëŠ”ê°€(Causal Lift)"ë¥¼ ë¶„ì„.

**ì‚¬ìš© ìŠ¤í‚¬**: `DoWhy`, `CausalML`, `21-multiagent`

#### ì—ì´ì „íŠ¸ êµ¬ì„±

1.  **Graph Agent**: ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì¸ê³¼ ê·¸ë˜í”„(Causal Graph) ì´ˆì•ˆ ìƒì„± (ex: ê°€ê²© â†’ êµ¬ë§¤, ë‚ ì”¨ â†’ êµ¬ë§¤).
2.  **Estimation Agent**: Double Machine Learning ë“±ìœ¼ë¡œ ì¸ê³¼ íš¨ê³¼(ATE) ì¶”ì •.
3.  **Refutation Agent**: ì¶”ì •ëœ ì¸ê³¼ íš¨ê³¼ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ ë°˜ë°•(Refutation) í…ŒìŠ¤íŠ¸ ìˆ˜í–‰.

```python
# ê°€ìƒì˜ Causal Library ì‚¬ìš©
import dowhy
from dowhy import CausalModel

class CausalAnalyst:
    def analyze_marketing_roi(self, df):
        # 1. ì¸ê³¼ ê·¸ë˜í”„ ì •ì˜ (LLM ë„ì›€)
        causal_graph = """
        digraph {
            Ads -> Sales;
            Seasonality -> Sales;
            Seasonality -> Ads;
            CompetitorPrice -> Sales;
        }
        """
        
        # 2. ëª¨ë¸ë§
        model = CausalModel(
            data=df,
            treatment='Ads',
            outcome='Sales',
            graph=causal_graph
        )
        
        # 3. ì‹ë³„ (Identification)
        identified_estimand = model.identify_effect()
        
        # 4. ì¶”ì • (Estimation)
        estimate = model.estimate_effect(
            identified_estimand,
            method_name="backdoor.linear_regression"
        )
        
        # 5. ê²€ì¦ (Refutation) - ì¤‘ìš”!
        refute = model.refute_estimate(
            identified_estimand,
            estimate,
            method_name="random_common_cause"
        )
        
        return {
            "causal_effect": estimate.value, # ìˆœìˆ˜ ê´‘ê³  íš¨ê³¼
            "robustness": refute.is_robust
        }
```

## H2. "Auto-Kaggle" ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸

### ì‹œë‚˜ë¦¬ì˜¤: ë°ì´í„°ì…‹ë§Œ ë˜ì ¸ì£¼ë©´ ë² ì´ìŠ¤ë¼ì¸ ì •ë³µ

**ëª©í‘œ**: `train.csv`, `test.csv`ë§Œ ì…ë ¥í•˜ë©´ EDAë¶€í„° ì „ì²˜ë¦¬, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§, ë³µìˆ˜ì˜ ëª¨ë¸ í•™ìŠµ(LGBM, XGBoost, CatBoost), ì•™ìƒë¸”, ê·¸ë¦¬ê³  ê²°ê³¼ ì œì¶œ íŒŒì¼ê¹Œì§€ ìƒì„±.

**ì‚¬ìš© ìŠ¤í‚¬**: `13-mlops`, `05-data-processing`, `Optuna`

#### êµ¬í˜„

```python
class AutoKaggler:
    def __init__(self, target_col):
        self.target = target_col
        
    def run_pipeline(self, train_path, test_path):
        # 1. ìë™ EDA ë° íƒ€ì… ì¶”ë¡ 
        df_train = pd.read_csv(train_path)
        col_types = self.infer_column_types(df_train)
        
        # 2. LLM ê¸°ë°˜ í”¼ì²˜ ì•„ì´ë””ì–´ ìƒì„±
        feature_ideas = self.brain.brainstorm_features(df_train.columns)
        # ex: "TransactionDateì—ì„œ 'ì£¼ë§ ì—¬ë¶€', 'ê³µíœ´ì¼ ì—¬ë¶€' íŒŒìƒ ë³€ìˆ˜ ìƒì„± ì¶”ì²œ"
        
        # 3. ì „ì²˜ë¦¬ ë° í”¼ì²˜ ìƒì„± ì½”ë“œ ì‹¤í–‰
        X_train, y_train = self.preprocessor.transform(df_train, feature_ideas)
        
        # 4. ëª¨ë¸ ì„ íƒ ë° HPO (Optuna)
        study = optuna.create_study(direction="maximize")
        study.optimize(lambda trial: self.objective(trial, X_train, y_train), n_trials=50)
        
        # 5. Stacking Ensemble
        best_models = self.get_top_k_models(study, k=3)
        stacker = StackingClassifier(estimators=best_models, final_estimator=LogisticRegression())
        stacker.fit(X_train, y_train)
        
        # 6. ë¦¬í¬íŠ¸ ìƒì„±
        return TrainingReport(
            cv_score=stacker.score,
            feature_importance=stacker.feature_importances_,
            submission_file="submission.csv"
        )
```

---

# Part I: í‹ˆìƒˆ ë„ë©”ì¸ íŠ¹í™”

## I1. íŠ¹í—ˆ ì¹¨í•´ ê°€ëŠ¥ì„± ë¶„ì„ê¸°

### ì‹œë‚˜ë¦¬ì˜¤: ê¸°ìˆ (Tech) + ë²•ë¥ (Legal) í•˜ì´ë¸Œë¦¬ë“œ

**ëª©í‘œ**: ê°œë°œ ì¤‘ì¸ ì œí’ˆì˜ ê¸°ìˆ  ëª…ì„¸ì„œ(Spec)ì™€ ê²½ìŸì‚¬ì˜ íŠ¹í—ˆ ë¬¸ì„œë¥¼ ë¹„êµí•˜ì—¬ ì¹¨í•´ ê°€ëŠ¥ì„±(Risk Score)ì„ ì‚°ì¶œí•˜ê³  íšŒí”¼ ì„¤ê³„(Design-around) ë°©ì•ˆ ì œì•ˆ.

**ì‚¬ìš© ìŠ¤í‚¬**: `15-rag` (íŠ¹í—ˆ DB), `03-fine-tuning` (ë²•ë¥  ìš©ì–´ í•™ìŠµ), `21-multiagent`

#### í”„ë¡œì„¸ìŠ¤

1.  **Claim Parser**: íŠ¹í—ˆì˜ ê¶Œë¦¬ ë²”ìœ„ì¸ 'ì²­êµ¬í•­(Claims)'ì„ êµ¬ì„±ìš”ì†Œë³„ë¡œ ë¶„í•´ (Element-by-element analysis).
2.  **Product Mapper**: ìš°ë¦¬ ì œí’ˆì˜ ê¸°ëŠ¥ì„ ì²­êµ¬í•­ êµ¬ì„±ìš”ì†Œì™€ ë§¤í•‘.
3.  **Risk Scorer**: ê° êµ¬ì„±ìš”ì†Œì˜ ì¼ì¹˜ ì—¬ë¶€(All-elements rule) íŒë‹¨. í•˜ë‚˜ë¼ë„ ë¶ˆì¼ì¹˜í•˜ë©´ ë¹„ì¹¨í•´.
4.  **Advisor**: ì¹¨í•´ ì†Œì§€ê°€ ìˆëŠ” êµ¬ì„±ìš”ì†Œë¥¼ ëŒ€ì²´í•  ê¸°ìˆ ì  ëŒ€ì•ˆ ì œì•ˆ.

```python
class PatentRiskAnalyzer:
    async def check_infringement(self, product_spec: str, patent_id: str):
        # íŠ¹í—ˆ ì²­êµ¬í•­ ë¡œë“œ
        claims = await self.patent_db.get_claims(patent_id)
        
        # êµ¬ì„±ìš”ì†Œ ë¶„í•´ (LLM)
        elements = await self.llm.invoke(f"ë‹¤ìŒ ì²­êµ¬í•­ì„ ë…ë¦½ëœ êµ¬ì„±ìš”ì†Œë¡œ ë¶„í•´í•´:\n{claims}")
        
        report = []
        infringement_flag = True
        
        for element in elements:
            # êµ¬ì„±ìš”ì†Œ ë§¤ì¹­
            match = await self.compare(element, product_spec)
            report.append(match)
            
            if match.status == "NOT_FOUND":
                infringement_flag = False # êµ¬ì„±ìš”ì†Œ ì™„ë¹„ ì›ì¹™ì— ì˜í•´ ë¹„ì¹¨í•´
        
        if infringement_flag:
            return {
                "risk": "HIGH", 
                "advice": await self.suggest_workaround(report)
            }
        else:
            return {"risk": "LOW"}
```

## I2. ê°œì¸í™”ëœ "Second Brain" ì§€ì‹ ê·¸ë˜í”„

### ì‹œë‚˜ë¦¬ì˜¤: ë¡œì»¬ ì§€ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ (Obsidian/Notion ì—°ë™)

**ëª©í‘œ**: ì‚¬ìš©ìì˜ ë©”ëª¨, ë¶ë§ˆí¬, ì¼ê¸°ë¥¼ ë²¡í„°í™”í•˜ì—¬ ì €ì¥ë§Œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, **ì§€ì‹ ê·¸ë˜í”„(Knowledge Graph)**ë¡œ ì—°ê²°í•˜ì—¬ "ì´ ì•„ì´ë””ì–´ëŠ” ì‘ë…„ì˜ ì € ìƒê°ê³¼ ì—°ê²°ë©ë‹ˆë‹¤"ë¼ê³  ì œì•ˆ.

**ì‚¬ìš© ìŠ¤í‚¬**: `15-rag`, `NetworkX` (ê·¸ë˜í”„), `Local LLM` (í”„ë¼ì´ë²„ì‹œ)

```python
class SecondBrain:
    def __init__(self):
        self.graph = KnowledgeGraph()
        self.vectordb = Chroma()
        
    def add_note(self, note: Note):
        # 1. í‚¤ì›Œë“œ ë° ì—”í‹°í‹° ì¶”ì¶œ
        entities = self.extract_entities(note.content)
        
        # 2. ë²¡í„° ì„ë² ë”© ì €ì¥
        self.vectordb.add(note)
        
        # 3. ê·¸ë˜í”„ ë…¸ë“œ/ì—£ì§€ ìƒì„±
        self.graph.add_node(note.id, type="Note")
        for entity in entities:
             self.graph.add_node(entity, type="Concept")
             self.graph.add_edge(note.id, entity, relation="mentions")
             
        # 4. ì—°ê²° ë°œê²¬ (Serendipity)
        related_notes = self.find_hidden_connections(note.id)
        return related_notes
    
    def find_hidden_connections(self, note_id):
        # ê·¸ë˜í”„ íƒìƒ‰: 2-hop neighbor ì¤‘ ê´€ë ¨ì„± ë†’ì€ ê²ƒ ì¶”ì²œ
        # A(ìƒˆ ë©”ëª¨) -> Concept X -> B(ê³¼ê±° ë©”ëª¨)
        pass
```

---

**ë²„ì „**: 1.0
**ìµœì¢… ìˆ˜ì •**: 2025-12-08
**ìœ ì§€ê´€ë¦¬**: Orchestra Research
