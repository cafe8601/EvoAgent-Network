# AI Research Skills í™•ì¥ ì˜ˆì‹œ ë¶ (Extended Cookbook)

> í˜„ì‹¤ì ì¸ í”„ë¡œì íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ì™€ ìƒì„¸ êµ¬í˜„ ê°€ì´ë“œ

---

## ğŸ“š ëª©ì°¨

### Part A: ì—°êµ¬ & í•™ìŠµ íŒŒì´í”„ë¼ì¸
- [A1. ë„ë©”ì¸ íŠ¹í™” LLM ê°œë°œ (ì˜ë£Œ/ë²•ë¥ /ê¸ˆìœµ)](#a1-ë„ë©”ì¸-íŠ¹í™”-llm-ê°œë°œ)
- [A2. ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ](#a2-ì—°ì†-í•™ìŠµ-ì‹œìŠ¤í…œ)
- [A3. ëª¨ë¸ ê²½ëŸ‰í™” íŒŒì´í”„ë¼ì¸](#a3-ëª¨ë¸-ê²½ëŸ‰í™”-íŒŒì´í”„ë¼ì¸)
- [A4. í•©ì„± ë°ì´í„° ìƒì„±](#a4-í•©ì„±-ë°ì´í„°-ìƒì„±)

### Part B: í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ
- [B1. ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì‹œìŠ¤í…œ](#b1-ì—”í„°í”„ë¼ì´ì¦ˆ-rag-ì‹œìŠ¤í…œ)
- [B2. ì‹¤ì‹œê°„ ì±„íŒ… ì„œë¹„ìŠ¤](#b2-ì‹¤ì‹œê°„-ì±„íŒ…-ì„œë¹„ìŠ¤)
- [B3. ë¬¸ì„œ ìë™í™” ì‹œìŠ¤í…œ](#b3-ë¬¸ì„œ-ìë™í™”-ì‹œìŠ¤í…œ)
- [B4. ì½˜í…ì¸  ìƒì„± í”Œë«í¼](#b4-ì½˜í…ì¸ -ìƒì„±-í”Œë«í¼)

### Part C: ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
- [C1. ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ìë™í™”](#c1-ì†Œí”„íŠ¸ì›¨ì–´-ê°œë°œ-ìë™í™”)
- [C2. ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸](#c2-ë°ì´í„°-ë¶„ì„-íŒŒì´í”„ë¼ì¸)
- [C3. ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜](#c3-ì½˜í…ì¸ -ëª¨ë”ë ˆì´ì…˜)
- [C4. ì—°êµ¬ ë…¼ë¬¸ ë¶„ì„](#c4-ì—°êµ¬-ë…¼ë¬¸-ë¶„ì„)

### Part D: í•œêµ­ ì‹œì¥ íŠ¸ë ˆì´ë”© ì‹¬í™”
- [D1. ì˜µì…˜ ìŠ¤í”„ë ˆë“œ ì „ëµ](#d1-ì˜µì…˜-ìŠ¤í”„ë ˆë“œ-ì „ëµ)
- [D2. ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë ˆì´ë”©](#d2-ë‰´ìŠ¤-ê¸°ë°˜-íŠ¸ë ˆì´ë”©)
- [D3. í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”](#d3-í¬íŠ¸í´ë¦¬ì˜¤-ìµœì í™”)

### Part E: DevOps & ì¸í”„ë¼
- [E1. MLOps íŒŒì´í”„ë¼ì¸](#e1-mlops-íŒŒì´í”„ë¼ì¸)
- [E2. ì„œë²„ë¦¬ìŠ¤ AI ë°°í¬](#e2-ì„œë²„ë¦¬ìŠ¤-ai-ë°°í¬)

---

# Part A: ì—°êµ¬ & í•™ìŠµ íŒŒì´í”„ë¼ì¸

## A1. ë„ë©”ì¸ íŠ¹í™” LLM ê°œë°œ

### ì‹œë‚˜ë¦¬ì˜¤: ê¸ˆìœµ ë„ë©”ì¸ LLM

**ëª©í‘œ**: ê¸ˆìœµ ë³´ê³ ì„œ ë¶„ì„, íˆ¬ì ë¦¬ì„œì¹˜ ìš”ì•½, ì¬ë¬´ì œí‘œ í•´ì„ì— íŠ¹í™”ëœ LLM ê°œë°œ

**ì‚¬ìš© ìŠ¤í‚¬**: `02`, `03`, `05`, `06`, `11`, `15`

#### Step 1: ê¸ˆìœµ ì½”í¼ìŠ¤ ìˆ˜ì§‘ ë° ì •ì œ

```python
# 05-data-processing/ray-data
import ray
from ray.data import read_json

# ë°ì´í„° ì†ŒìŠ¤
sources = [
    "sec_filings/",      # SEC ê³µì‹œ ìë£Œ
    "earnings_calls/",   # ì‹¤ì  ë°œí‘œ ì „ì‚¬ë³¸
    "analyst_reports/",  # ì• ë„ë¦¬ìŠ¤íŠ¸ ë¦¬í¬íŠ¸
    "financial_news/"    # ê¸ˆìœµ ë‰´ìŠ¤
]

@ray.remote
def process_financial_doc(doc):
    # 1. ê¸ˆìœµ ìš©ì–´ í‘œì¤€í™”
    doc = standardize_financial_terms(doc)
    
    # 2. ìˆ«ì/í†µí™” ì •ê·œí™”
    doc = normalize_numbers(doc)
    
    # 3. í…Œì´ë¸” ì¶”ì¶œ ë° êµ¬ì¡°í™”
    tables = extract_tables(doc)
    
    # 4. í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
    quality_score = compute_quality(doc)
    
    return {
        "text": doc,
        "tables": tables,
        "quality_score": quality_score,
        "source": doc["source"]
    }

# ë¶„ì‚° ì²˜ë¦¬
ds = ray.data.read_json(sources)
processed = ds.map(process_financial_doc)
processed.filter(lambda x: x["quality_score"] > 0.7).write_parquet("financial_corpus/")
```

#### Step 2: ê¸ˆìœµ í† í¬ë‚˜ì´ì € í™•ì¥

```python
# 02-tokenization
from transformers import AutoTokenizer

base_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-8B")

# ê¸ˆìœµ íŠ¹ìˆ˜ í† í° ì¶”ê°€
financial_tokens = [
    # í†µí™”
    "â‚©", "â‚¬", "Â£", "Â¥",
    # ê¸ˆìœµ ìš©ì–´
    "EBITDA", "P/E", "EPS", "ROE", "ROA", "WACC",
    "M&A", "IPO", "ETF", "SPAC",
    # ìˆ«ì í‘œí˜„
    "1Q", "2Q", "3Q", "4Q",
    "YoY", "QoQ", "MoM",
    # ë“±ê¸‰
    "AAA", "AA+", "AA", "AA-", "A+", "A", "A-",
]

base_tokenizer.add_tokens(financial_tokens)
base_tokenizer.save_pretrained("./financial_tokenizer")
```

#### Step 3: ë„ë©”ì¸ SFT

```yaml
# axolotl_financial_sft.yaml
base_model: meta-llama/Meta-Llama-3-8B
tokenizer_type: LlamaTokenizer
load_in_4bit: true

datasets:
  - path: ./financial_instructions.jsonl
    type: alpaca
  - path: ./earnings_qa.jsonl
    type: alpaca
  - path: ./report_summary.jsonl
    type: alpaca

output_dir: ./financial_llm_sft
sequence_len: 4096

adapter: lora
lora_r: 64
lora_alpha: 128
lora_target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj

learning_rate: 2e-5
num_epochs: 3
micro_batch_size: 2
gradient_accumulation_steps: 8

wandb_project: financial-llm
```

#### Step 4: ê¸ˆìœµ RAG í†µí•©

```python
# 15-rag + 14-agents
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# ê¸ˆìœµ ë¬¸ì„œ ë²¡í„° DB
financial_db = Chroma(
    persist_directory="./financial_vectordb",
    embedding_function=HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
)

# ê¸ˆìœµ íŠ¹í™” í”„ë¡¬í”„íŠ¸
FINANCIAL_PROMPT = """
ë‹¹ì‹ ì€ ê¸ˆìœµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

ê·œì¹™:
1. ìˆ˜ì¹˜ ë°ì´í„°ëŠ” ì •í™•í•˜ê²Œ ì¸ìš©í•˜ì„¸ìš”
2. ì¶œì²˜(ë¬¸ì„œëª…, ë‚ ì§œ)ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
3. ë¶ˆí™•ì‹¤í•œ ì •ë³´ëŠ” ëª…í™•íˆ í‘œì‹œí•˜ì„¸ìš”
4. íˆ¬ì ì¡°ì–¸ì´ ì•„ë‹˜ì„ ëª…ì‹œí•˜ì„¸ìš”

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:
"""

llm = ChatOpenAI(model="./financial_llm_sft", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=financial_db.as_retriever(search_kwargs={"k": 5}),
    chain_type_kwargs={"prompt": FINANCIAL_PROMPT}
)
```

---

## A2. ì—°ì† í•™ìŠµ ì‹œìŠ¤í…œ

### ì‹œë‚˜ë¦¬ì˜¤: ë‰´ìŠ¤ ì½”í¼ìŠ¤ë¡œ ì§€ì†ì  ëª¨ë¸ ì—…ë°ì´íŠ¸

**ëª©í‘œ**: ìƒˆë¡œìš´ ì§€ì‹ì„ ì§€ì†ì ìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ ê¸°ì¡´ ì§€ì‹ ìœ ì§€

**ì‚¬ìš© ìŠ¤í‚¬**: `03`, `05`, `11`, `13`

#### ì•„í‚¤í…ì²˜

```
[ìƒˆ ë°ì´í„° ìŠ¤íŠ¸ë¦¼] â†’ [í’ˆì§ˆ í•„í„°] â†’ [ë§ê° í‰ê°€] 
                                      â†“
                              [ì ì§„ì  í•™ìŠµ]
                                      â†“
                              [ì„±ëŠ¥ ê²€ì¦]
                                      â†“
                    [Pass] â†’ [ëª¨ë¸ ì—…ë°ì´íŠ¸] â†’ [ë°°í¬]
                    [Fail] â†’ [ë¡¤ë°±]
```

#### êµ¬í˜„

```python
class ContinualLearningPipeline:
    def __init__(self, base_model_path: str):
        self.model = AutoModelForCausalLM.from_pretrained(base_model_path)
        self.replay_buffer = ReplayBuffer(max_size=10000)
        self.evaluator = BenchmarkEvaluator(["mmlu", "hellaswag"])
        self.baseline_scores = self.evaluator.evaluate(self.model)
        
    def update(self, new_data: Dataset):
        # 1. ìƒˆ ë°ì´í„° í’ˆì§ˆ ê²€ì¦
        filtered_data = self.quality_filter(new_data)
        
        # 2. ë¦¬í”Œë ˆì´ ë²„í¼ì—ì„œ ê¸°ì¡´ ë°ì´í„° ìƒ˜í”Œë§ (ë§ê° ë°©ì§€)
        replay_data = self.replay_buffer.sample(len(filtered_data) // 2)
        combined_data = concatenate_datasets([filtered_data, replay_data])
        
        # 3. ì ì§„ì  í•™ìŠµ (ì‘ì€ learning rate)
        trainer = Trainer(
            model=self.model,
            train_dataset=combined_data,
            args=TrainingArguments(
                learning_rate=1e-6,  # ë‚®ì€ LR
                num_train_epochs=1,
                per_device_train_batch_size=4,
                output_dir="./continual_checkpoints"
            )
        )
        trainer.train()
        
        # 4. ì„±ëŠ¥ ê²€ì¦ (ë§ê° ì²´í¬)
        new_scores = self.evaluator.evaluate(self.model)
        
        forgetting_rate = self._calculate_forgetting(
            self.baseline_scores, new_scores
        )
        
        if forgetting_rate > 0.05:  # 5% ì´ìƒ ì„±ëŠ¥ ì €í•˜
            print(f"âš ï¸ ë§ê° ê°ì§€: {forgetting_rate:.1%} ì„±ëŠ¥ ì €í•˜")
            self._rollback()
            return False
        
        # 5. ë¦¬í”Œë ˆì´ ë²„í¼ ì—…ë°ì´íŠ¸
        self.replay_buffer.add(filtered_data)
        
        # 6. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        self.model.save_pretrained(f"./model_v{self.version}")
        self.version += 1
        
        return True
    
    def _calculate_forgetting(self, old_scores, new_scores):
        """ê° íƒœìŠ¤í¬ë³„ ë§ê°ë¥  ê³„ì‚°"""
        forgetting = {}
        for task in old_scores:
            if old_scores[task] > 0:
                drop = (old_scores[task] - new_scores[task]) / old_scores[task]
                forgetting[task] = max(0, drop)
        return sum(forgetting.values()) / len(forgetting)
```

---

## A3. ëª¨ë¸ ê²½ëŸ‰í™” íŒŒì´í”„ë¼ì¸

### ì‹œë‚˜ë¦¬ì˜¤: 70B ëª¨ë¸ì„ ëª¨ë°”ì¼ì—ì„œ ì‹¤í–‰

**ëª©í‘œ**: í”„ë¡œë•ì…˜ ëª¨ë¸ì„ ì—£ì§€ ë””ë°”ì´ìŠ¤ìš©ìœ¼ë¡œ ê²½ëŸ‰í™”

**ì‚¬ìš© ìŠ¤í‚¬**: `10`, `19`, `12`

#### ê²½ëŸ‰í™” ì „ëµ

```
[ì›ë³¸ 70B ëª¨ë¸]
      â†“
[Knowledge Distillation] â†’ [7B í•™ìƒ ëª¨ë¸]
      â†“
[Pruning] â†’ 30% íŒŒë¼ë¯¸í„° ì œê±°
      â†“
[Quantization] â†’ 4-bit GGUF
      â†“
[ë²¤ì¹˜ë§ˆí¬] â†’ ì •í™•ë„ 95% ìœ ì§€ í™•ì¸
      â†“
[ë°°í¬] â†’ llama.cpp / iOS / Android
```

#### Step 1: ì§€ì‹ ì¦ë¥˜

```python
# 19-emerging-techniques/knowledge-distillation
from transformers import AutoModelForCausalLM
import torch

class DistillationTrainer:
    def __init__(self, teacher_model, student_model, temperature=2.0):
        self.teacher = teacher_model
        self.teacher.eval()
        self.student = student_model
        self.temperature = temperature
        
    def distillation_loss(self, student_logits, teacher_logits, labels):
        # Soft target loss (KL Divergence)
        soft_teacher = F.softmax(teacher_logits / self.temperature, dim=-1)
        soft_student = F.log_softmax(student_logits / self.temperature, dim=-1)
        soft_loss = F.kl_div(soft_student, soft_teacher, reduction='batchmean')
        
        # Hard target loss (Cross Entropy)
        hard_loss = F.cross_entropy(student_logits, labels)
        
        # Combined loss
        return 0.7 * soft_loss * (self.temperature ** 2) + 0.3 * hard_loss
    
    def train_step(self, batch):
        with torch.no_grad():
            teacher_outputs = self.teacher(**batch)
            teacher_logits = teacher_outputs.logits
        
        student_outputs = self.student(**batch)
        student_logits = student_outputs.logits
        
        loss = self.distillation_loss(
            student_logits, 
            teacher_logits, 
            batch["labels"]
        )
        
        return loss
```

#### Step 2: ì–‘ìí™”

```bash
# GGUF ë³€í™˜ (llama.cpp)
python convert.py ./distilled_7b --outtype f16 --outfile distilled_7b.f16.gguf

# 4-bit ì–‘ìí™”
./quantize distilled_7b.f16.gguf distilled_7b.Q4_K_M.gguf Q4_K_M

# í¬ê¸° í™•ì¸
# ì›ë³¸ 70B: ~140GB
# ì¦ë¥˜ 7B: ~14GB  
# 4-bit: ~4GB
```

#### Step 3: ëª¨ë°”ì¼ ë°°í¬

```swift
// iOS (llama.cpp Swift binding)
import LlamaCpp

class MobileLLM {
    private var context: LlamaContext?
    
    func load(modelPath: String) {
        let params = LlamaContextParams.default()
        params.n_ctx = 2048
        params.n_threads = 4  // ëª¨ë°”ì¼ ìµœì í™”
        
        context = LlamaContext(path: modelPath, params: params)
    }
    
    func generate(prompt: String) async -> String {
        guard let ctx = context else { return "" }
        
        let tokens = ctx.tokenize(prompt)
        var output = ""
        
        for _ in 0..<256 { // max tokens
            let nextToken = ctx.sample()
            if nextToken == ctx.eosToken { break }
            output += ctx.detokenize([nextToken])
        }
        
        return output
    }
}
```

---

## A4. í•©ì„± ë°ì´í„° ìƒì„±

### ì‹œë‚˜ë¦¬ì˜¤: ê°œì¸ì •ë³´ ì—†ëŠ” í•™ìŠµ ë°ì´í„° ìƒì„±

**ëª©í‘œ**: í”„ë¼ì´ë²„ì‹œ ë³´í˜¸í•˜ë©´ì„œ í•™ìŠµìš© ë°ì´í„° ìƒì„±

**ì‚¬ìš© ìŠ¤í‚¬**: `03`, `07`, `11`

```python
class SyntheticDataGenerator:
    """ê°œì¸ì •ë³´ ì œê±°ëœ í•©ì„± ë°ì´í„° ìƒì„±"""
    
    def __init__(self, generator_model, validator_model):
        self.generator = generator_model
        self.validator = validator_model
        self.pii_detector = PIIDetector()
        
    def generate_batch(self, template: str, n_samples: int) -> List[Dict]:
        samples = []
        
        for _ in range(n_samples):
            # 1. í•©ì„± ë°ì´í„° ìƒì„±
            generated = self.generator.generate(template)
            
            # 2. PII ê²€ì¶œ
            pii_found = self.pii_detector.detect(generated)
            if pii_found:
                # PII ì œê±°/ëŒ€ì²´
                generated = self.pii_detector.anonymize(generated)
            
            # 3. í’ˆì§ˆ ê²€ì¦
            quality_score = self.validator.score(generated)
            
            if quality_score > 0.8:
                samples.append({
                    "text": generated,
                    "quality": quality_score,
                    "pii_clean": not pii_found
                })
        
        return samples
    
    def generate_instruction_dataset(self, categories: List[str], per_category: int):
        """ì§€ì‹œì‚¬í•­ ë°ì´í„°ì…‹ ìƒì„±"""
        
        dataset = []
        
        for category in categories:
            template = CATEGORY_TEMPLATES[category]
            
            for i in range(per_category):
                # ë‹¤ì–‘í•œ ë³µì¡ë„
                complexity = ["simple", "medium", "complex"][i % 3]
                
                instruction = self.generator.generate(
                    f"Generate a {complexity} {category} instruction"
                )
                
                response = self.generator.generate(
                    f"Instruction: {instruction}\nResponse:"
                )
                
                # í’ˆì§ˆ ê²€ì¦
                if self.validator.validate(instruction, response):
                    dataset.append({
                        "instruction": instruction,
                        "input": "",
                        "output": response,
                        "category": category,
                        "complexity": complexity
                    })
        
        return dataset

# ì‚¬ìš© ì˜ˆì‹œ
generator = SyntheticDataGenerator(
    generator_model=load_model("gpt-4o"),
    validator_model=load_model("claude-3-opus")
)

# 10,000ê°œ í•©ì„± ë°ì´í„° ìƒì„±
synthetic_data = generator.generate_instruction_dataset(
    categories=["coding", "math", "writing", "reasoning"],
    per_category=2500
)
```

---

# Part B: í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ

## B1. ì—”í„°í”„ë¼ì´ì¦ˆ RAG ì‹œìŠ¤í…œ

### ì‹œë‚˜ë¦¬ì˜¤: ëŒ€ê¸°ì—… ì‚¬ë‚´ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ

**ëª©í‘œ**: 10ë§Œ+ ë¬¸ì„œ, 1000+ ë™ì‹œ ì‚¬ìš©ì ì§€ì›

**ì‚¬ìš© ìŠ¤í‚¬**: `15`, `14`, `12`, `17`, `07`

#### ì•„í‚¤í…ì²˜

```
[ì‚¬ìš©ì] â†’ [API Gateway] â†’ [Load Balancer]
                                 â†“
                    [RAG Service Cluster]
                    /         |         \
            [vLLM 1]    [vLLM 2]    [vLLM 3]
                    \         |         /
                     [Vector DB Cluster]
                     (Qdrant / Milvus)
                              â†“
                    [Document Store]
                    (PostgreSQL + S3)
```

#### êµ¬í˜„

```python
# FastAPI + ë¹„ë™ê¸° RAG
from fastapi import FastAPI, BackgroundTasks
from qdrant_client import QdrantClient
from openai import AsyncOpenAI
import asyncio

app = FastAPI()
qdrant = QdrantClient(host="qdrant-cluster", port=6333)
llm_client = AsyncOpenAI(base_url="http://vllm-cluster:8000/v1")

class EnterpriseRAG:
    def __init__(self):
        self.cache = RedisCache()
        self.rate_limiter = RateLimiter(requests_per_minute=100)
        
    async def search(self, query: str, user_id: str, department: str):
        # 1. ìºì‹œ í™•ì¸
        cache_key = f"rag:{hash(query)}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        # 2. ACL ê¸°ë°˜ í•„í„°ë§
        access_filter = self.build_access_filter(user_id, department)
        
        # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Semantic + Keyword)
        semantic_results = await self.semantic_search(query, access_filter)
        keyword_results = await self.keyword_search(query, access_filter)
        
        # 4. Reciprocal Rank Fusion
        merged = self.rrf_merge(semantic_results, keyword_results)
        
        # 5. LLM ìƒì„±
        context = self.format_context(merged[:5])
        response = await self.generate_response(query, context)
        
        # 6. ìºì‹œ ì €ì¥
        await self.cache.set(cache_key, response, ttl=3600)
        
        return response
    
    async def semantic_search(self, query: str, filter: dict):
        embedding = await self.embed(query)
        
        results = await qdrant.search(
            collection_name="enterprise_docs",
            query_vector=embedding,
            query_filter=filter,
            limit=10
        )
        
        return results
    
    async def generate_response(self, query: str, context: str):
        messages = [
            {"role": "system", "content": ENTERPRISE_SYSTEM_PROMPT},
            {"role": "user", "content": f"Context:\n{context}\n\nQuestion: {query}"}
        ]
        
        response = await llm_client.chat.completions.create(
            model="meta-llama/Meta-Llama-3-70B-Instruct",
            messages=messages,
            temperature=0.1,
            max_tokens=1024
        )
        
        return response.choices[0].message.content

@app.post("/api/v1/search")
async def search_endpoint(request: SearchRequest, background_tasks: BackgroundTasks):
    rag = EnterpriseRAG()
    
    # Rate limiting
    if not rag.rate_limiter.allow(request.user_id):
        raise HTTPException(status_code=429, detail="Rate limit exceeded")
    
    response = await rag.search(
        query=request.query,
        user_id=request.user_id,
        department=request.department
    )
    
    # ë°±ê·¸ë¼ìš´ë“œë¡œ ë¡œê¹…
    background_tasks.add_task(log_query, request, response)
    
    return {"answer": response, "sources": response.sources}
```

---

## B2. ì‹¤ì‹œê°„ ì±„íŒ… ì„œë¹„ìŠ¤

### ì‹œë‚˜ë¦¬ì˜¤: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ + ëŒ€í™” ê¸°ë¡ ê´€ë¦¬

**ì‚¬ìš© ìŠ¤í‚¬**: `12`, `14`, `17`

```python
# WebSocket ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…
from fastapi import FastAPI, WebSocket
from openai import AsyncOpenAI
import json

app = FastAPI()
client = AsyncOpenAI(base_url="http://vllm:8000/v1")

class ChatSession:
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.messages = []
        self.redis = Redis()
        
    async def load(self):
        """ì„¸ì…˜ ë³µì›"""
        data = await self.redis.get(f"chat:{self.session_id}")
        if data:
            self.messages = json.loads(data)
    
    async def save(self):
        """ì„¸ì…˜ ì €ì¥"""
        await self.redis.setex(
            f"chat:{self.session_id}",
            3600 * 24,  # 24ì‹œê°„ ìœ ì§€
            json.dumps(self.messages)
        )
    
    def add_message(self, role: str, content: str):
        self.messages.append({"role": role, "content": content})
        
        # ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ê´€ë¦¬ (ìµœê·¼ 20ê°œë§Œ)
        if len(self.messages) > 20:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ìœ ì§€ + ìµœê·¼ 19ê°œ
            self.messages = self.messages[:1] + self.messages[-19:]

@app.websocket("/ws/chat/{session_id}")
async def chat_websocket(websocket: WebSocket, session_id: str):
    await websocket.accept()
    
    session = ChatSession(session_id)
    await session.load()
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if not session.messages:
        session.add_message("system", "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.")
    
    try:
        while True:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ìˆ˜ì‹ 
            user_message = await websocket.receive_text()
            session.add_message("user", user_message)
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            stream = await client.chat.completions.create(
                model="meta-llama/Meta-Llama-3-8B-Instruct",
                messages=session.messages,
                stream=True,
                max_tokens=1024
            )
            
            full_response = ""
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    token = chunk.choices[0].delta.content
                    full_response += token
                    
                    # ì‹¤ì‹œê°„ í† í° ì „ì†¡
                    await websocket.send_json({
                        "type": "token",
                        "content": token
                    })
            
            # ì™„ë£Œ ì‹ í˜¸
            await websocket.send_json({
                "type": "complete",
                "content": full_response
            })
            
            session.add_message("assistant", full_response)
            await session.save()
            
    except Exception as e:
        await websocket.send_json({
            "type": "error",
            "content": str(e)
        })
```

---

## B3. ë¬¸ì„œ ìë™í™” ì‹œìŠ¤í…œ

### ì‹œë‚˜ë¦¬ì˜¤: ê³„ì•½ì„œ/ë³´ê³ ì„œ ìë™ ìƒì„±

**ì‚¬ìš© ìŠ¤í‚¬**: `16`, `15`, `07`

```python
from pydantic import BaseModel
from instructor import from_openai
from openai import OpenAI

# êµ¬ì¡°í™”ëœ ì¶œë ¥ (16-prompt-engineering/instructor)
client = from_openai(OpenAI())

class ContractSection(BaseModel):
    title: str
    content: str
    legal_references: List[str]

class Contract(BaseModel):
    parties: List[str]
    effective_date: str
    sections: List[ContractSection]
    signatures: List[str]

class ContractGenerator:
    def __init__(self):
        self.template_db = ContractTemplateDB()
        self.legal_rag = LegalRAG()
        
    def generate(self, contract_type: str, params: dict) -> Contract:
        # 1. í…œí”Œë¦¿ ë¡œë“œ
        template = self.template_db.get(contract_type)
        
        # 2. ê´€ë ¨ ë²•ë¥  ì¡°í•­ ê²€ìƒ‰
        legal_context = self.legal_rag.search(
            f"{contract_type} ê´€ë ¨ ë²•ë¥  ì¡°í•­"
        )
        
        # 3. êµ¬ì¡°í™”ëœ ê³„ì•½ì„œ ìƒì„±
        contract = client.chat.completions.create(
            model="gpt-4o",
            response_model=Contract,
            messages=[
                {"role": "system", "content": f"""
                    ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                    ë‹¤ìŒ í…œí”Œë¦¿ê³¼ ë²•ë¥  ì¡°í•­ì„ ì°¸ê³ í•˜ì—¬ ê³„ì•½ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
                    
                    í…œí”Œë¦¿: {template}
                    ê´€ë ¨ ë²•ë¥ : {legal_context}
                """},
                {"role": "user", "content": f"""
                    ê³„ì•½ ìœ í˜•: {contract_type}
                    ë‹¹ì‚¬ì: {params['parties']}
                    ê³„ì•½ ì¡°ê±´: {params['terms']}
                """}
            ]
        )
        
        # 4. ê²€ì¦
        self.validate(contract)
        
        return contract
    
    def validate(self, contract: Contract):
        """ê³„ì•½ì„œ ìœ íš¨ì„± ê²€ì¦"""
        # í•„ìˆ˜ ì¡°í•­ ì¡´ì¬ í™•ì¸
        required_sections = ["ëª©ì ", "ê¸°ê°„", "ëŒ€ê¸ˆ", "í•´ì§€"]
        for req in required_sections:
            if not any(req in s.title for s in contract.sections):
                raise ValueError(f"í•„ìˆ˜ ì¡°í•­ ëˆ„ë½: {req}")
```

---

## B4. ì½˜í…ì¸  ìƒì„± í”Œë«í¼

### ì‹œë‚˜ë¦¬ì˜¤: ë§ˆì¼€íŒ… ì½˜í…ì¸  ìë™ ìƒì„±

**ì‚¬ìš© ìŠ¤í‚¬**: `14`, `16`, `18`

```python
class ContentGenerationPlatform:
    """ë©€í‹°ì±„ë„ ë§ˆì¼€íŒ… ì½˜í…ì¸  ìƒì„±"""
    
    def __init__(self):
        self.text_llm = ChatOpenAI(model="gpt-4o")
        self.image_gen = DallE3()
        self.voice_gen = ElevenLabs()
        
    async def generate_campaign(self, brief: str, channels: List[str]):
        """ìº í˜ì¸ë³„ ì½˜í…ì¸  ìƒì„±"""
        
        results = {}
        
        for channel in channels:
            if channel == "instagram":
                results["instagram"] = await self.generate_instagram(brief)
            elif channel == "blog":
                results["blog"] = await self.generate_blog(brief)
            elif channel == "email":
                results["email"] = await self.generate_email(brief)
            elif channel == "video_script":
                results["video"] = await self.generate_video_script(brief)
        
        return results
    
    async def generate_instagram(self, brief: str):
        # ìº¡ì…˜ ìƒì„±
        caption = await self.text_llm.ainvoke(f"""
            ë‹¤ìŒ ë¸Œë¦¬í”„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¸ìŠ¤íƒ€ê·¸ë¨ ìº¡ì…˜ì„ ì‘ì„±í•˜ì„¸ìš”.
            - 150ì ì´ë‚´
            - ì´ëª¨ì§€ í™œìš©
            - í•´ì‹œíƒœê·¸ 5ê°œ í¬í•¨
            
            ë¸Œë¦¬í”„: {brief}
        """)
        
        # ì´ë¯¸ì§€ ìƒì„±
        image_prompt = await self.text_llm.ainvoke(f"""
            ë‹¤ìŒ ìº¡ì…˜ì— ì–´ìš¸ë¦¬ëŠ” ì´ë¯¸ì§€ë¥¼ DALL-Eë¡œ ìƒì„±í•˜ê¸° ìœ„í•œ 
            ì˜ì–´ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            
            ìº¡ì…˜: {caption}
        """)
        
        image_url = await self.image_gen.generate(image_prompt)
        
        return {
            "caption": caption,
            "image_url": image_url,
            "suggested_posting_time": self.get_best_posting_time("instagram")
        }
    
    async def generate_blog(self, brief: str):
        # SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ ê¸€
        outline = await self.text_llm.ainvoke(f"""
            ë‹¤ìŒ ì£¼ì œë¡œ SEO ìµœì í™”ëœ ë¸”ë¡œê·¸ ê¸€ ê°œìš”ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
            - H1, H2, H3 êµ¬ì¡°
            - íƒ€ê²Ÿ í‚¤ì›Œë“œ ì œì•ˆ
            - ë©”íƒ€ ì„¤ëª… í¬í•¨
            
            ì£¼ì œ: {brief}
        """)
        
        full_content = await self.text_llm.ainvoke(f"""
            ë‹¤ìŒ ê°œìš”ë¥¼ ë°”íƒ•ìœ¼ë¡œ 2000ì ë¶„ëŸ‰ì˜ ë¸”ë¡œê·¸ ê¸€ì„ ì‘ì„±í•˜ì„¸ìš”.
            
            ê°œìš”: {outline}
        """)
        
        return {
            "title": outline["title"],
            "content": full_content,
            "meta_description": outline["meta"],
            "keywords": outline["keywords"]
        }
```

---

# Part C: ë©€í‹°ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ

## C1. ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ ìë™í™”

### ì‹œë‚˜ë¦¬ì˜¤: ìš”êµ¬ì‚¬í•­ â†’ ì½”ë“œ â†’ í…ŒìŠ¤íŠ¸ â†’ ë°°í¬

**ì‚¬ìš© ìŠ¤í‚¬**: `21`, `24`, `14`

```python
class SoftwareDevTeam:
    """AI ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ íŒ€"""
    
    def __init__(self):
        self.agents = {
            "pm": ProjectManagerAgent(),
            "architect": ArchitectAgent(),
            "developer": DeveloperAgent(),
            "reviewer": CodeReviewerAgent(),
            "tester": TesterAgent(),
            "devops": DevOpsAgent()
        }
        self.memory = SharedMemory()
        
    async def develop_feature(self, requirement: str):
        """ê¸°ëŠ¥ ê°œë°œ ì „ì²´ íŒŒì´í”„ë¼ì¸"""
        
        # 1. PM: ìš”êµ¬ì‚¬í•­ ë¶„ì„ ë° íƒœìŠ¤í¬ ë¶„í•´
        tasks = await self.agents["pm"].analyze(requirement)
        self.memory.set("tasks", tasks)
        
        # 2. Architect: ê¸°ìˆ  ì„¤ê³„
        design = await self.agents["architect"].design(
            requirement, 
            tasks,
            existing_code=self.memory.get("codebase")
        )
        self.memory.set("design", design)
        
        # 3. Developer: ì½”ë“œ êµ¬í˜„ (TDD)
        for task in tasks:
            # í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‘ì„±
            tests = await self.agents["tester"].write_tests(task, design)
            
            # ì½”ë“œ êµ¬í˜„
            code = await self.agents["developer"].implement(
                task, 
                design, 
                tests
            )
            
            # ì½”ë“œ ë¦¬ë·°
            review = await self.agents["reviewer"].review(code, tests)
            
            if review.issues:
                # ë¦¬ë·° ë°˜ì˜
                code = await self.agents["developer"].fix(code, review.issues)
            
            self.memory.append("implemented", {"task": task, "code": code})
        
        # 4. í†µí•© í…ŒìŠ¤íŠ¸
        integration_results = await self.agents["tester"].integration_test(
            self.memory.get("implemented")
        )
        
        if not integration_results.passed:
            # ë¬¸ì œ í•´ê²°
            fixes = await self.agents["developer"].fix_integration(
                integration_results.failures
            )
        
        # 5. ë°°í¬ ì¤€ë¹„
        deployment = await self.agents["devops"].prepare_deployment(
            self.memory.get("implemented")
        )
        
        return {
            "tasks_completed": len(tasks),
            "code": self.memory.get("implemented"),
            "tests": integration_results,
            "deployment_ready": deployment
        }
```

---

## C2. ë°ì´í„° ë¶„ì„ íŒŒì´í”„ë¼ì¸

### ì‹œë‚˜ë¦¬ì˜¤: ìì—°ì–´ â†’ SQL â†’ ì¸ì‚¬ì´íŠ¸

**ì‚¬ìš© ìŠ¤í‚¬**: `21`, `16`, `14`

```python
class DataAnalystTeam:
    """ë°ì´í„° ë¶„ì„ ë©€í‹°ì—ì´ì „íŠ¸"""
    
    def __init__(self, db_connection):
        self.db = db_connection
        self.agents = {
            "interpreter": QueryInterpreterAgent(),
            "sql_writer": SQLWriterAgent(db_schema=self.get_schema()),
            "executor": QueryExecutorAgent(db_connection),
            "analyst": InsightAnalystAgent(),
            "visualizer": VisualizerAgent()
        }
    
    async def analyze(self, question: str) -> AnalysisResult:
        # 1. ìì—°ì–´ í•´ì„
        intent = await self.agents["interpreter"].interpret(question)
        
        # 2. SQL ìƒì„±
        sql = await self.agents["sql_writer"].generate(intent)
        
        # 3. SQL ê²€ì¦ ë° ì‹¤í–‰
        if await self.validate_sql(sql):
            results = await self.agents["executor"].execute(sql)
        else:
            # SQL ìˆ˜ì • ìš”ì²­
            sql = await self.agents["sql_writer"].fix(sql, validation_errors)
            results = await self.agents["executor"].execute(sql)
        
        # 4. ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        insights = await self.agents["analyst"].extract_insights(
            question=question,
            data=results,
            context=self.get_business_context()
        )
        
        # 5. ì‹œê°í™” ìƒì„±
        charts = await self.agents["visualizer"].create_charts(
            data=results,
            insights=insights
        )
        
        return AnalysisResult(
            query=sql,
            data=results,
            insights=insights,
            visualizations=charts
        )
    
    async def validate_sql(self, sql: str) -> bool:
        """SQL ì•ˆì „ì„± ê²€ì¦"""
        # EXPLAINìœ¼ë¡œ ì‹¤í–‰ ê³„íš í™•ì¸
        # ìœ„í—˜í•œ íŒ¨í„´ ê°ì§€ (DROP, TRUNCATE ë“±)
        # ì„±ëŠ¥ ì˜ˆì¸¡
        pass
```

---

## C3. ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜

### ì‹œë‚˜ë¦¬ì˜¤: ì‚¬ìš©ì ì½˜í…ì¸  ì‹¤ì‹œê°„ ê²€ìˆ˜

**ì‚¬ìš© ìŠ¤í‚¬**: `21`, `07`, `18`

```python
class ContentModerationPipeline:
    """ì½˜í…ì¸  ëª¨ë”ë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.agents = {
            "text": TextModerationAgent(),
            "image": ImageModerationAgent(),
            "spam": SpamDetectionAgent(),
            "context": ContextAnalyzerAgent(),
            "appeals": AppealsHandlerAgent()
        }
        self.policy = CommunityPolicy.load("./policies/community_guidelines.yaml")
        
    async def moderate(self, content: UserContent) -> ModerationResult:
        # ë³‘ë ¬ ê²€ì‚¬
        checks = await asyncio.gather(
            self.agents["text"].check(content.text) if content.text else None,
            self.agents["image"].check(content.images) if content.images else None,
            self.agents["spam"].check(content),
            return_exceptions=True
        )
        
        text_result, image_result, spam_result = checks
        
        # ê²°ê³¼ ì·¨í•©
        violations = []
        
        if text_result and text_result.violations:
            violations.extend(text_result.violations)
        
        if image_result and image_result.violations:
            violations.extend(image_result.violations)
        
        if spam_result and spam_result.is_spam:
            violations.append(Violation(type="spam", confidence=spam_result.confidence))
        
        # ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ (í’ì, ì¸ìš© ë“±)
        if violations:
            context_check = await self.agents["context"].analyze(
                content=content,
                violations=violations
            )
            
            # ì»¨í…ìŠ¤íŠ¸ë¡œ í•´ì œ ê°€ëŠ¥í•œ ìœ„ë°˜ í•„í„°ë§
            violations = [v for v in violations if not context_check.is_exception(v)]
        
        # ìµœì¢… íŒì •
        if not violations:
            return ModerationResult(action="approve")
        
        severity = max(v.severity for v in violations)
        
        if severity >= 0.9:
            return ModerationResult(action="remove", violations=violations)
        elif severity >= 0.7:
            return ModerationResult(action="review", violations=violations)
        else:
            return ModerationResult(action="warn", violations=violations)
```

---

## C4. ì—°êµ¬ ë…¼ë¬¸ ë¶„ì„

### ì‹œë‚˜ë¦¬ì˜¤: ë…¼ë¬¸ ìë™ ë¦¬ë·° ë° ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ

**ì‚¬ìš© ìŠ¤í‚¬**: `21`, `15`, `18`

```python
class ResearchPaperAnalyzer:
    """ì—°êµ¬ ë…¼ë¬¸ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.agents = {
            "parser": PaperParserAgent(),
            "methodology": MethodologyReviewerAgent(),
            "stats": StatisticalReviewerAgent(),
            "literature": LiteratureLinkerAgent(),
            "summarizer": SummarizerAgent(),
            "critic": CriticAgent()
        }
        self.arxiv_db = ArxivVectorDB()
        
    async def analyze_paper(self, pdf_bytes: bytes) -> PaperAnalysis:
        # 1. ë…¼ë¬¸ íŒŒì‹±
        paper = await self.agents["parser"].parse(pdf_bytes)
        
        # 2. ë³‘ë ¬ ë¶„ì„
        results = await asyncio.gather(
            self.agents["methodology"].review(paper.methodology),
            self.agents["stats"].review(paper.results, paper.tables),
            self.agents["literature"].find_related(paper.abstract, self.arxiv_db),
            self.agents["critic"].critique(paper)
        )
        
        methodology_review, stats_review, related_papers, critique = results
        
        # 3. ì¢…í•© ìš”ì•½
        summary = await self.agents["summarizer"].summarize(
            paper=paper,
            reviews={
                "methodology": methodology_review,
                "statistics": stats_review,
                "critique": critique
            },
            related=related_papers
        )
        
        return PaperAnalysis(
            title=paper.title,
            tldr=summary.tldr,  # 1ë¬¸ì¥ ìš”ì•½
            key_contributions=summary.contributions,
            methodology_assessment=methodology_review,
            statistical_validity=stats_review,
            limitations=critique.limitations,
            related_work=related_papers[:5],
            recommended_for=self.classify_audience(paper)
        )
```

---

# Part D: í•œêµ­ ì‹œì¥ íŠ¸ë ˆì´ë”© ì‹¬í™”

## D1. ì˜µì…˜ ìŠ¤í”„ë ˆë“œ ì „ëµ

### ì‹œë‚˜ë¦¬ì˜¤: ì•„ì´ì–¸ ì½˜ë„ë¥´ ìë™ë§¤ë§¤

```python
class IronCondorStrategy:
    """ì•„ì´ì–¸ ì½˜ë„ë¥´ ìë™ë§¤ë§¤ ì „ëµ"""
    
    def __init__(self, api):
        self.api = api
        self.config = {
            "wing_width": 2,      # í–‰ì‚¬ê°€ ê°„ê²© (ATM ê¸°ì¤€)
            "target_delta": 0.15, # íƒ€ê²Ÿ ë¸íƒ€
            "profit_target": 0.5, # 50% ìµì ˆ
            "stop_loss": 2.0,     # ìµœëŒ€ ì†ì‹¤ 2x í”„ë¦¬ë¯¸ì—„
            "dte_range": (7, 21)  # ì”ì¡´ì¼ 7-21ì¼
        }
        
    def find_strikes(self, chain: OptionChain):
        """ì ì • í–‰ì‚¬ê°€ íƒìƒ‰"""
        atm = chain.get_atm_strike()
        
        # ë¸íƒ€ ê¸°ì¤€ìœ¼ë¡œ OTM í–‰ì‚¬ê°€ ì„ íƒ
        short_call = chain.find_strike_by_delta(0.15, "call")
        short_put = chain.find_strike_by_delta(-0.15, "put")
        
        long_call = short_call + self.config["wing_width"] * chain.strike_interval
        long_put = short_put - self.config["wing_width"] * chain.strike_interval
        
        return {
            "short_call": short_call,
            "long_call": long_call,
            "short_put": short_put,
            "long_put": long_put
        }
    
    def calculate_greeks(self, strikes, chain):
        """í¬ì§€ì…˜ ê·¸ë¦­ìŠ¤ ê³„ì‚°"""
        total_delta = 0
        total_gamma = 0
        total_theta = 0
        total_vega = 0
        
        # Short Call Spread
        total_delta -= chain.get_option(strikes["short_call"], "call").delta
        total_delta += chain.get_option(strikes["long_call"], "call").delta
        
        # Short Put Spread
        total_delta -= chain.get_option(strikes["short_put"], "put").delta
        total_delta += chain.get_option(strikes["long_put"], "put").delta
        
        # Gamma, Theta, Vega ìœ ì‚¬í•˜ê²Œ ê³„ì‚°
        # ...
        
        return {
            "delta": total_delta,
            "gamma": total_gamma,
            "theta": total_theta,
            "vega": total_vega
        }
    
    def entry_signal(self, market_data):
        """ì§„ì… ì‹œê·¸ë„"""
        # IVê°€ ë†’ì„ ë•Œ ì§„ì… (IV Rank > 50)
        iv_rank = self.calculate_iv_rank(market_data)
        
        if iv_rank < 50:
            return False
        
        # íš¡ë³´ì¥ í™•ì¸ (ADX < 25)
        adx = self.calculate_adx(market_data, period=14)
        
        if adx > 25:
            return False
        
        return True
    
    def manage_position(self, position):
        """í¬ì§€ì…˜ ê´€ë¦¬"""
        current_pnl = position.unrealized_pnl
        max_profit = position.initial_credit
        
        # 50% ìµì ˆ
        if current_pnl >= max_profit * 0.5:
            return {"action": "close", "reason": "profit_target"}
        
        # ìŠ¤í†±ë¡œìŠ¤
        if current_pnl <= -max_profit * 2:
            return {"action": "close", "reason": "stop_loss"}
        
        # ì”ì¡´ì¼ 3ì¼ ì´í•˜: ì²­ì‚°
        if position.dte <= 3:
            return {"action": "close", "reason": "expiration"}
        
        # ë¸íƒ€ ì´íƒˆ ì‹œ ì¡°ì •
        greeks = self.calculate_greeks(position.strikes, position.chain)
        if abs(greeks["delta"]) > 0.2:
            return {"action": "adjust", "reason": "delta_breach"}
        
        return {"action": "hold"}
```

---

## D2. ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë ˆì´ë”©

### ì‹œë‚˜ë¦¬ì˜¤: ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ â†’ íŠ¸ë ˆì´ë”© ì‹œê·¸ë„

```python
class NewsBasedTrading:
    """ë‰´ìŠ¤ ê¸°ë°˜ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.news_stream = NaverNewsStream()
        self.sentiment_model = load_model("./finance_sentiment_ko")
        self.entity_extractor = FinanceNER()
        self.position_manager = PositionManager()
        
    async def process_news(self, news: NewsArticle):
        # 1. ê´€ë ¨ ì¢…ëª© ì¶”ì¶œ
        entities = self.entity_extractor.extract(news.content)
        stocks = [e for e in entities if e.type == "STOCK"]
        
        if not stocks:
            return
        
        # 2. ê°ì„± ë¶„ì„
        sentiment = self.sentiment_model.predict(news.content)
        
        # 3. ë‰´ìŠ¤ ì¤‘ìš”ë„ íŒë‹¨
        importance = self.calculate_importance(news)
        
        # 4. ì‹œê·¸ë„ ìƒì„±
        for stock in stocks:
            signal = self.generate_signal(stock, sentiment, importance)
            
            if signal.strength >= 0.7:
                await self.execute_signal(stock, signal)
    
    def calculate_importance(self, news: NewsArticle):
        """ë‰´ìŠ¤ ì¤‘ìš”ë„ ê³„ì‚°"""
        factors = {
            "source_credibility": self.get_source_score(news.source),
            "breaking": 1.5 if "ì†ë³´" in news.title else 1.0,
            "market_hours": 1.2 if self.is_market_hours() else 0.8,
            "first_report": 1.3 if not self.is_duplicate(news) else 0.5
        }
        
        return sum(factors.values()) / len(factors)
    
    def generate_signal(self, stock, sentiment, importance):
        # ê¸ì • ë‰´ìŠ¤ + ë†’ì€ ì¤‘ìš”ë„ = ë§¤ìˆ˜
        if sentiment.label == "positive" and importance > 0.8:
            return Signal(
                direction="buy",
                strength=sentiment.confidence * importance,
                holding_period="short"  # ë‹¨ê¸°
            )
        
        # ë¶€ì • ë‰´ìŠ¤ = ë§¤ë„/ê³µë§¤ë„
        if sentiment.label == "negative" and importance > 0.8:
            return Signal(
                direction="sell",
                strength=sentiment.confidence * importance,
                holding_period="short"
            )
        
        return Signal(direction="hold", strength=0)
```

---

## D3. í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”

### ì‹œë‚˜ë¦¬ì˜¤: AI ê¸°ë°˜ ìì‚° ë°°ë¶„

```python
class PortfolioOptimizer:
    """AI ê¸°ë°˜ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”"""
    
    def __init__(self):
        self.return_predictor = ReturnPredictionModel()
        self.risk_model = RiskModel()
        
    def optimize(self, assets: List[str], constraints: dict):
        # 1. ìˆ˜ìµë¥  ì˜ˆì¸¡
        expected_returns = self.return_predictor.predict(assets)
        
        # 2. ê³µë¶„ì‚° í–‰ë ¬ ì¶”ì •
        cov_matrix = self.risk_model.estimate_covariance(assets)
        
        # 3. ìµœì í™” (Mean-Variance + Constraints)
        from scipy.optimize import minimize
        
        def objective(weights):
            portfolio_return = np.dot(weights, expected_returns)
            portfolio_risk = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
            
            # Sharpe Ratio ìµœëŒ€í™”
            return -portfolio_return / portfolio_risk
        
        # ì œì•½ì¡°ê±´
        cons = [
            {"type": "eq", "fun": lambda x: np.sum(x) - 1},  # í•© = 1
        ]
        
        # ìì‚°ë³„ ë¹„ì¤‘ ì œí•œ
        bounds = [(constraints.get("min_weight", 0), 
                   constraints.get("max_weight", 0.3)) for _ in assets]
        
        result = minimize(
            objective,
            x0=np.ones(len(assets)) / len(assets),
            method="SLSQP",
            bounds=bounds,
            constraints=cons
        )
        
        return {
            "weights": dict(zip(assets, result.x)),
            "expected_return": np.dot(result.x, expected_returns),
            "expected_risk": np.sqrt(np.dot(result.x.T, np.dot(cov_matrix, result.x))),
            "sharpe_ratio": -result.fun
        }
```

---

# Part E: DevOps & ì¸í”„ë¼

## E1. MLOps íŒŒì´í”„ë¼ì¸

### ì‹œë‚˜ë¦¬ì˜¤: ìë™í™”ëœ ëª¨ë¸ í•™ìŠµ-í‰ê°€-ë°°í¬

**ì‚¬ìš© ìŠ¤í‚¬**: `13`, `11`, `12`

```yaml
# .github/workflows/mlops.yml
name: MLOps Pipeline

on:
  push:
    paths:
      - 'training/**'
      - 'data/**'

jobs:
  train:
    runs-on: [self-hosted, gpu]
    steps:
      - uses: actions/checkout@v4
      
      - name: Train Model
        run: |
          python training/train.py \
            --config training/config.yaml \
            --output models/
        env:
          WANDB_API_KEY: ${{ secrets.WANDB_KEY }}
      
      - name: Evaluate
        run: |
          python evaluation/evaluate.py \
            --model models/latest \
            --benchmarks mmlu,hellaswag
      
      - name: Check Performance
        run: |
          python scripts/check_metrics.py \
            --threshold 0.75 \
            --metric accuracy
      
      - name: Build Container
        if: success()
        run: |
          docker build -t llm-service:${{ github.sha }} .
          docker push registry/llm-service:${{ github.sha }}
      
      - name: Deploy to Staging
        if: success()
        run: |
          kubectl set image deployment/llm-staging \
            llm=registry/llm-service:${{ github.sha }}
      
      - name: Run E2E Tests
        run: |
          pytest tests/e2e/ --staging-url $STAGING_URL
      
      - name: Deploy to Production
        if: github.ref == 'refs/heads/main'
        run: |
          kubectl set image deployment/llm-prod \
            llm=registry/llm-service:${{ github.sha }}
```

---

## E2. ì„œë²„ë¦¬ìŠ¤ AI ë°°í¬

### ì‹œë‚˜ë¦¬ì˜¤: Modalë¡œ ì˜¨ë””ë§¨ë“œ GPU ì„œë¹™

**ì‚¬ìš© ìŠ¤í‚¬**: `09`, `12`

```python
# 09-infrastructure/modal
import modal

app = modal.App("llm-service")

# GPU ì´ë¯¸ì§€ ì •ì˜
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install("vllm", "torch", "transformers")
    .run_commands("apt-get update && apt-get install -y git")
)

# ëª¨ë¸ ë³¼ë¥¨
model_volume = modal.Volume.from_name("llm-models", create_if_missing=True)

@app.cls(
    gpu=modal.gpu.A100(count=2),
    image=image,
    volumes={"/models": model_volume},
    container_idle_timeout=300,  # 5ë¶„ ìœ íœ´ í›„ ì¢…ë£Œ
    concurrency_limit=10
)
class LLMService:
    @modal.enter()
    def load_model(self):
        from vllm import LLM
        
        self.llm = LLM(
            model="/models/Meta-Llama-3-70B-Instruct-AWQ",
            tensor_parallel_size=2,
            quantization="awq",
            max_model_len=8192
        )
    
    @modal.method()
    def generate(self, prompt: str, max_tokens: int = 1024):
        from vllm import SamplingParams
        
        params = SamplingParams(
            temperature=0.7,
            max_tokens=max_tokens
        )
        
        outputs = self.llm.generate([prompt], params)
        return outputs[0].outputs[0].text

@app.function(schedule=modal.Cron("0 * * * *"))  # ë§¤ì‹œê°„
def warmup():
    """ì½œë“œ ìŠ¤íƒ€íŠ¸ ë°©ì§€ë¥¼ ìœ„í•œ ì›Œë°ì—…"""
    service = LLMService()
    service.generate.remote("Hello", max_tokens=10)

# ë¡œì»¬ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    with app.run():
        service = LLMService()
        result = service.generate.remote("ì•ˆë…•í•˜ì„¸ìš”, ìê¸°ì†Œê°œ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
        print(result)
```

---

## ë¶€ë¡: ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)

| ë¬¸ì œ | í•´ê²°ì±… |
|------|--------|
| í•™ìŠµ ì‹œ OOM | `gradient_checkpointing`, `batch_size` ê°ì†Œ |
| ì¶”ë¡  ì‹œ OOM | `quantization` (4-bit), `max_model_len` ê°ì†Œ |
| vLLM OOM | `gpu_memory_utilization` ì¡°ì • (0.9 â†’ 0.8) |

### ëŠë¦° ì¶”ë¡  ì†ë„

| ë¬¸ì œ | í•´ê²°ì±… |
|------|--------|
| ì²« í† í° ëŠë¦¼ | KV ìºì‹œ ì›Œë°ì—…, í”„ë¦¬í•„ ìµœì í™” |
| ì „ì²´ ëŠë¦¼ | `tensor_parallel_size` ì¦ê°€, Flash Attention |
| ë°°ì¹˜ ë¹„íš¨ìœ¨ | `continuous_batching` í™œì„±í™” |

### í•™ìŠµ ë¶ˆì•ˆì •

| ë¬¸ì œ | í•´ê²°ì±… |
|------|--------|
| Loss ë°œì‚° | `learning_rate` ê°ì†Œ, `gradient_clipping` |
| Loss ì •ì²´ | `learning_rate` ì¡°ì •, ë°ì´í„° í’ˆì§ˆ í™•ì¸ |
| ê³¼ì í•© | `dropout` ì¦ê°€, ë°ì´í„° ì¦ê°•, ì¡°ê¸° ì¢…ë£Œ |

---

**ë²„ì „**: 1.0
**ìµœì¢… ìˆ˜ì •**: 2025-12-08
**ìœ ì§€ê´€ë¦¬**: Orchestra Research
