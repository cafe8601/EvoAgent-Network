# 🏗️ Part 1: AI 모델링 기초 (Foundations)

> **"AI 모델링의 시작점: 아키텍처 이해부터 데이터 파이프라인까지"**
>
> 이 가이드북은 AI 연구의 가장 기초가 되는 **모델 구조(01)**, **토큰화(02)**, **데이터 처리(05)** 스킬을 다룹니다.

---

## 📑 목차
- [01. Model Architecture (모델 아키텍처)](#01-model-architecture-모델-아키텍처)
- [02. Tokenization (토큰화)](#02-tokenization-토큰화)
- [05. Data Processing (데이터 처리)](#05-data-processing-데이터-처리)
- [⬅️ 메인으로 돌아가기](./GUIDEBOOK_INDEX.md)

---

## 01. Model Architecture (모델 아키텍처)

### 📖 설명
트랜스포머, Mamba(상태 공간 모델), RWKV 등 다양한 모델 아키텍처를 처음부터 구현하고 이해하기 위한 스킬입니다. 모델의 뼈대를 직접 만져보며 원리를 체득합니다.

### 🎯 언제 사용하나요?
- 새로운 모델 아키텍처를 학습하고 싶을 때
- GPT 스타일 모델을 처음부터 구현할 때
- 기존 아키텍처를 커스터마이징할 때 (예: Attention 메커니즘 변경)
- 연구 목적으로 모델 구조를 실험할 때

### 📦 핵심 도구
| 도구 | 설명 | 사용 사례 |
|------|------|----------|
| **NanoGPT** | 미니멀 GPT 구현 (Karpathy) | 교육용, 개념 이해 |
| **LitGPT** | 프로덕션급 LLM 프레임워크 | 실제 학습/서빙, Llama/Mistral 지원 |
| **Mamba** | 상태 공간 모델 (SSM) | 긴 시퀀스 처리, 선형 복잡도 |
| **RWKV** | RNN-Transformer 하이브리드 | 효율적 추론, RNN의 메모리 효율성 |
| **Megatron-Core** | NVIDIA 대규모 학습 | 엔터프라이즈급 대규모 모델 학습 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: 교육용 GPT 구현
"NanoGPT를 사용해서 작은 GPT-2 모델을 처음부터 학습시키고 싶어.
Shakespeare 텍스트 데이터로 학습하는 방법을 알려줘."

🔹 시나리오 2: 아키텍처 비교 연구
"Mamba 아키텍처의 원리를 설명하고,
트랜스포머 대비 어떤 장점이 있는지 코드 예제와 함께 보여줘."

🔹 시나리오 3: 프로덕션 파인튜닝
"LitGPT로 Llama-3 8B 모델을 로드하고 fine-tuning하는
전체 파이프라인을 구현해줘. LoRA 설정을 포함해줘."
```

### 🔗 연계 스킬
- **→ 02-Tokenization**: 모델에 맞는 토크나이저 설계
- **→ 08-Distributed Training**: 대규모 모델 분산 학습
- **→ 10-Optimization**: 학습/추론 최적화

---

## 02. Tokenization (토큰화)

### 📖 설명
텍스트를 모델이 처리할 수 있는 토큰으로 변환하는 토크나이저를 학습하고 사용합니다. 언어 이해의 첫 단추입니다.

### 🎯 언제 사용하나요?
- 커스텀 토크나이저를 학습해야 할 때 (예: 새로운 언어 지원)
- 한국어 등 특정 언어에 최적화된 토크나이저가 필요할 때
- 기존 토크나이저의 vocab을 확장할 때 (도메인 용어 추가)
- BPE, WordPiece, Unigram 등 알고리즘을 이해하고 싶을 때

### 📦 핵심 도구
| 도구 | 설명 | 사용 사례 |
|------|------|----------|
| **HuggingFace Tokenizers** | 고성능 Rust 기반 라이브러리 | 대부분의 LLM 사용 사례 |
| **SentencePiece** | Google의 언어 독립적 토크나이저 | 다국어 지원, Llama 계열 사용 |
| **Tiktoken** | OpenAI의 고속 BPE 토크나이저 | GPT 계열 모델 사용 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: 한국어 전용 토크나이저
"한국어 코퍼스로 BPE 토크나이저를 처음부터 학습시키는 방법을 알려줘.
vocab 사이즈는 32000으로 설정하고 싶어."

🔹 시나리오 2: 도메인 용어 추가
"HuggingFace tokenizers 라이브러리로 기존 Llama 토크나이저에
한국어 금융 용어(EBITDA, KOSPI 등)를 추가하는 방법을 보여줘."

🔹 시나리오 3: 알고리즘 비교
"SentencePiece와 HuggingFace Tokenizers의 차이점을 설명하고,
어떤 상황에서 어떤 것을 사용해야 하는지 알려줘."
```

### 🔗 연계 스킬
- **→ 01-Model Architecture**: 아키텍처에 맞는 vocab 크기 설정
- **→ 05-Data Processing**: 토크나이저 학습을 위한 데이터 정제

---

## 05. Data Processing (데이터 처리)

### 📖 설명
대규모 AI 학습 데이터를 수집, 정제, 필터링, 중복 제거하는 데이터 파이프라인 스킬입니다. TB급 데이터도 처리할 수 있는 확장성을 가집니다.

### 🎯 언제 사용하나요?
- 웹 크롤링 데이터를 정제해야 할 때 (HTML 제거, 텍스트 추출)
- 대규모 데이터셋의 중복을 제거할 때 (MinHash, LSH)
- 품질 필터링 (언어 감지, 유해성 필터, PII 제거)을 적용할 때
- 분산 처리로 TB급 데이터를 효율적으로 처리할 때

### 📦 핵심 도구
| 도구 | 설명 | 사용 사례 |
|------|------|----------|
| **NeMo Curator** | NVIDIA LLM 데이터 큐레이션 | 품질 필터링, 중복 제거, PII 마스킹 |
| **Ray Data** | 분산 데이터 처리 프레임워크 | 대규모 병렬 처리, 스트리밍 처리 |
| **Trafilatura** | 고성능 웹 텍스트 추출 | 웹 크롤링 데이터 파싱 |
| **Datasketch** | 확률적 중복 제거 | MinHashLSH 구현 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: 대규모 중복 제거
"NeMo Curator를 사용해서 Common Crawl 데이터를 정제하는 파이프라인을 구현해줘.
중복 제거, 언어 필터링, 품질 점수 필터링을 포함해야 해."

🔹 시나리오 2: 분산 처리 파이프라인
"Ray Data로 1TB 규모의 텍스트 데이터를 분산 처리해서
Parquet 형식으로 저장하는 방법을 알려줘."

🔹 시나리오 3: 한국어 데이터 정제
"한국어 웹 크롤링 데이터에서 광고, 스팸, 중복 콘텐츠를
제거하는 전처리 파이프라인을 구현해줘. Trafilatura를 사용해줘."
```

### 🔗 연계 스킬
- **→ 02-Tokenization**: 정제된 데이터로 토크나이저 학습
- **→ 03-Fine-tuning**: 학습을 위한 데이터셋 포맷팅 (JSONL 등)

---
