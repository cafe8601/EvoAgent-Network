# 🏋️ Part 2: 학습 파이프라인 (Training)

> **"모델 학습의 정수: 미세조정부터 대규모 분산 학습까지"**
>
> 이 가이드북은 AI 모델을 학습시키고(03, 06, 08), 최적화(10)하며, 평가(11)하는 핵심 파이프라인을 다룹니다.

---

## 📑 목차
- [03. Fine-tuning (미세조정)](#03-fine-tuning-미세조정)
- [06. Post-training (후처리 학습)](#06-post-training-후처리-학습)
- [08. Distributed Training (분산 학습)](#08-distributed-training-분산-학습)
- [10. Optimization (최적화)](#10-optimization-최적화)
- [11. Evaluation (평가)](#11-evaluation-평가)
- [⬅️ 메인으로 돌아가기](./GUIDEBOOK_INDEX.md)

---

## 03. Fine-tuning (미세조정)

### 📖 설명
사전 학습된 모델을 특정 태스크나 도메인에 맞게 미세조정하는 스킬입니다. LoRA, QLoRA 등 효율적인 기법을 사용하여 제한된 리소스로도 고성능 모델을 만들 수 있습니다.

### 🎯 언제 사용하나요?
- 기존 LLM을 특정 도메인(의료, 법률, 코드)에 맞게 조정할 때
- 제한된 GPU 메모리(Consumer GPU)로 큰 모델을 학습해야 할 때
- 채팅, 요약, 번역 등 특정 태스크에 특화된 모델을 만들 때
- LoRA/QLoRA로 빠르고 효율적인 학습을 원할 때

### 📦 핵심 도구
| 도구 | 설명 | 장점 |
|------|------|------|
| **Unsloth** | 2-5x 빠른 학습 라이브러리 | 메모리 효율, 극도의 속도 최적화 |
| **Axolotl** | YAML 기반 설정 프레임워크 | 다양한 모델/데이터셋 지원, 유연성 |
| **LLaMA-Factory** | WebUI 제공 통합 도구 | 사용 편의성, 100+ 모델 지원 |
| **PEFT** | HuggingFace 효율적 튜닝 | 표준 LoRA/QLoRA 구현 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: 의료 도메인 특화 (Unsloth)
"Unsloth를 사용해서 Llama-3 8B 모델을 QLoRA로 fine-tuning하고 싶어.
의료 도메인 데이터셋을 사용해서 학습하는 전체 과정을 알려줘."

🔹 시나리오 2: 한국어 대화 모델 (Axolotl)
"Axolotl 설정 파일(yml)을 작성해서 Mistral-7B를
한국어 대화 데이터로 SFT 학습시키는 방법을 보여줘."

🔹 시나리오 3: 제한된 리소스
"16GB GPU에서 7B 모델을 fine-tuning할 수 있는
가장 효율적인 방법과 설정을 추천해줘."
```

### 🔗 연계 스킬
- **→ 06-Post-training**: SFT 이후 인간 선호도 정렬 수행
- **→ 11-Evaluation**: 학습된 모델 성능 평가

---

## 06. Post-training (후처리 학습)

### 📖 설명
SFT(Supervised Fine-Tuning) 이후, RLHF(인간 피드백 강화학습), DPO(직접 선호 최적화) 등으로 모델을 인간의 선호도에 맞게 정렬하는 스킬입니다.

### 🎯 언제 사용하나요?
- 모델이 더 자연스럽고 도움이 되는 응답을 하도록 만들 때
- 유해하거나 편향된 출력을 줄이고 싶을 때
- 선호도 데이터셋(Chosen/Rejected)을 활용해 성능을 높일 때
- ChatGPT 스타일의 대화형 모델을 완성할 때

### 📦 핵심 도구
| 도구 | 설명 | 적합한 상황 |
|------|------|------------|
| **TRL** | HuggingFace RL 라이브러리 | PPO, DPO, ORPO 등 표준 RLHF 구현 |
| **GRPO** | 그룹 상대 정책 최적화 | 추론 능력 강화, 수학/코딩 문제 해결 |
| **OpenRLHF** | 대규모 RLHF 프레임워크 | 70B+ 모델 분산 RLHF |
| **SimPO** | 단순화된 선호도 최적화 | 레퍼런스 모델 없이 빠른 최적화 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: DPO 학습
"TRL 라이브러리로 DPO(Direct Preference Optimization) 학습을 구현해줘.
선호도 데이터셋 형식과 학습 설정을 자세히 설명해줘."

🔹 시나리오 2: 수학 추론 강화 (GRPO)
"GRPO를 사용해서 수학 문제 해결 능력을 향상시키는
강화학습 파이프라인을 구현해줘. 보상 함수 설정도 알려줘."

🔹 시나리오 3: 대규모 PPO
"OpenRLHF로 70B 모델에 PPO 학습을 적용하는 방법을 알려줘.
멀티 GPU 분산 학습 설정도 포함해줘."
```

### 🔗 연계 스킬
- **→ 07-Safety Alignment**: 안전 가드레일과 결합
- **→ 11-Evaluation**: 정렬된 모델의 승률(Win-rate) 평가

---

## 08. Distributed Training (분산 학습)

### 📖 설명
여러 GPU 또는 여러 노드에서 대규모 모델을 효율적으로 학습하기 위한 기술입니다. 70B 이상의 거대 모델을 학습하거나 학습 속도를 획기적으로 높일 때 사용합니다.

### 🎯 언제 사용하나요?
- 단일 GPU 메모리에 들어가지 않는 큰 모델을 학습할 때
- 학습 시간을 단축하기 위해 데이터 병렬 처리가 필요할 때
- ZeRO, FSDP 등 최신 메모리 최적화 기법을 적용할 때
- 엔터프라이즈급 대규모 학습 인프라를 구축할 때

### 📦 핵심 도구
| 도구 | 설명 | 특징 |
|------|------|------|
| **DeepSpeed** | Microsoft 분산 학습 라이브러리 | ZeRO Optimizer, 3D Parallelism |
| **PyTorch FSDP** | PyTorch 네이티브 완전 샤딩 | 별도 라이브러리 없이 사용 가능, 높은 호환성 |
| **Accelerate** | HuggingFace 분산 학습 래퍼 | 복잡한 설정을 간단하게 추상화 |
| **Ray Train** | Ray 기반 분산 학습 | 클러스터 관리와 결합된 학습 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: ZeRO-3 학습
"DeepSpeed ZeRO-3를 사용해서 13B 모델을 4개 GPU에서 학습하는
설정 파일(ds_config.json)과 학습 스크립트를 작성해줘."

🔹 시나리오 2: 멀티노드 학습
"PyTorch FSDP로 Llama-3 70B를 8개 노드에서
학습하는 방법을 알려줘. CPU offloading도 사용하고 싶어."

🔹 시나리오 3: 기존 코드 전환
"Accelerate 라이브러리로 기존 PyTorch 단일 GPU 코드를
최소한의 수정으로 멀티 GPU 학습으로 전환하는 방법을 보여줘."
```

### 🔗 연계 스킬
- **→ 09-Infrastructure**: 분산 학습을 위한 클러스터 구성
- **→ 13-MLOps**: 분산 학습 실험 로그 추적

---

## 10. Optimization (최적화)

### 📖 설명
모델의 학습 및 추론 속도를 높이고 GPU 메모리 사용량을 최소화하는 기술입니다. 하드웨어의 성능을 극한까지 끌어올립니다.

### 🎯 언제 사용하나요?
- 긴 컨텍스트(Long Context) 처리 시 OOM(Out of Memory)을 방지할 때
- 학습 및 추론 속도를 2배 이상 높이고 싶을 때
- 4-bit, 8-bit 양자화로 모델 사이즈를 줄일 때
- Flash Attention 등 최신 커널을 적용할 때

### 📦 핵심 도구
| 도구 | 설명 | 효과 |
|------|------|------|
| **Flash Attention** | IO 인식 Attention 알고리즘 | 메모리 사용량 감소, 속도 2-4배 향상 |
| **bitsandbytes** | 8-bit/4-bit 양자화 라이브러리 | 메모리 사용량 1/2~1/4로 감소 |
| **GPTQ / AWQ** | 사후 학습 양자화 기법 | 추론 속도 향상, 정확도 보존 |
| **Unsloth** | 통합 최적화 라이브러리 | 커널 레벨 최적화로 전체 성능 향상 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: Flash Attention 적용
"Flash Attention 2를 기존 PyTorch 코드에 통합하는 방법을 알려줘.
필요한 패키지 설치와 코드 수정 부분을 짚어줘."

🔹 시나리오 2: 4-bit 양자화 학습
"bitsandbytes로 Llama-3 70B를 4-bit로 로드하여 QLoRA 학습을
수행하는 코드를 작성해줘."

🔹 시나리오 3: 추론 최적화
"AWQ를 사용해서 모델을 양자화하고 vLLM에서 서빙할 때
성능(토큰/초)을 최적화하는 설정을 알려줘."
```

### 🔗 연계 스킬
- **→ 12-Inference Serving**: 최적화된 모델 서빙
- **→ 01-Model Architecture**: 효율적인 아키텍처 설계

---

## 11. Evaluation (평가)

### 📖 설명
LLM의 성능을 정량적, 정성적으로 평가하는 스킬입니다. 표준 벤치마크(MMLU, GSM8K)부터 사용자 정의 평가까지 포함합니다.

### 🎯 언제 사용하나요?
- 학습된 모델의 성능을 객관적으로 증명해야 할 때
- 베이스 모델과 파인튜닝 모델의 성능을 비교할 때
- 리더보드 점수를 측정하고 싶을 때
- 특정 도메인(코딩, 수학, 한국어) 능력을 검증할 때

### 📦 핵심 도구
| 도구 | 설명 | 사용 사례 |
|------|------|----------|
| **lm-evaluation-harness** | 표준 평가 프레임워크 (EleutherAI) | MMLU, HellaSwag 등 60+ 벤치마크 |
| **MT-Bench** | 멀티턴 대화 평가 (LLM-as-a-Judge) | 챗봇 성능 평가 |
| **HumanEval** | 코딩 능력 평가 | 코드 생성 모델 평가 |
| **LogicKor** | 한국어 논리 추론 벤치마크 | 한국어 모델 평가 |

### 💡 사용 예시 프롬프트

```markdown
🔹 시나리오 1: 표준 벤치마크 실행
"lm-evaluation-harness로 내 모델을 MMLU, GSM8K, HumanEval에서
평가하는 명령어를 알려줘. vLLM 백엔드를 사용하고 싶어."

🔹 시나리오 2: 지속적 평가 파이프라인
"학습 중 체크포인트가 저장될 때마다 자동으로 벤치마크를 실행하고
W&B에 결과를 기록하는 평가 파이프라인을 구현해줘."

🔹 시나리오 3: 한국어 능력 평가
"LogicKor 벤치마크를 사용해서 한국어 모델의 
추론 능력과 글쓰기 능력을 평가하는 방법을 보여줘."
```

### 🔗 연계 스킬
- **→ 13-MLOps**: 평가 결과 시각화 및 버전 관리
- **→ 17-Observability**: 실시간 응답 품질 평가
